{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 457A - Cooperative and Adaptive Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligence\n",
    "\n",
    "- **Intelligence**: The ability to acquire and apply knowledge and skills.\n",
    "- **Artificial Intelligence**: The science of creating intelligent machines, including intelligent computer programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rational Thinking & Rational Behavior\n",
    "\n",
    "- **Rational System**: A logical system that optimizes a given set of criteria.\n",
    "- **Rational Thinking**: A logical system that achieves goals via logical inferencing.\n",
    "- **Rational Behavior**: A logical system that perceives its environment and acts to achieve goals according to some set of beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "- **Agent**: Senses its environment and acts on collected information.\n",
    "- **Rational Agent**: An agent that acts in a way that is expected to maximize performance on the basis of perceptual history and built-in knowledge.\n",
    "\n",
    "#### Types of Agents\n",
    "\n",
    "- **Simple Reflex Agents**: Follow a lookup-table approach; needs fully observable environment.\n",
    "- **Model-Based Reflex Agents**: Add state information to handle partially observable environments.\n",
    "- **Goal-Based Agents**: Add concept of goals to help choose actions.\n",
    "- **Utility-Based Agents**: Add utility to decide \"good\" or \"bad\" when face with conflicting goals.\n",
    "- **Learning Agents**: Add ability to learn from experience to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "- **Fully vs. Partially Observable**:\n",
    "    - *Fully Observable*: Sensors can detect all aspects relevant to the choice of an action.\n",
    "    - *Partially Observable*: Missing Information or Inaccurate Sensors.\n",
    "- **Deterministic vs. Stochastic**:\n",
    "    - *Deterministic*: Environments that are only influenced by their current state and the next action executed by the agent.\n",
    "    - *Stochastic*: Randomness/Noise.\n",
    "- **Episodic vs. Sequential**:\n",
    "    - *Episodic*: The choice of an action in each episode does not depend on previous episodes.\n",
    "    - *Sequential*: An agent is required to \"think ahead\".\n",
    "- **Static vs. Dynamic**: N/A.\n",
    "- **Discrete vs. Continuous**: N/A.\n",
    "- **Single Agent vs. Multi Agent**: N/A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooperative and Adaptive Algorithms\n",
    "\n",
    "- **Cooperative**: Solve Joint Problems.\n",
    "- **Adaptive**: Change Behavior While Running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Search Problems\n",
    "\n",
    "- Large, Non-Polynomial Search Space Size\n",
    "- Large, Non-Polynomial Constraints Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well-Structured vs. Ill-Structured Problems\n",
    "\n",
    "- **Well-Structured Problems**: Problems for which the existing state and desired state are clearly identified, and the methods to reach the desired state are fairly obvious.\n",
    "- **Ill-Structured Problems**: Situation in which its existing state and the desired state are unclear and, hence, methods of reaching the desired state cannot be found.\n",
    "    1. Start & Improve Guess\n",
    "    2. Search Alternatives\n",
    "    4. Forward Search from Problem to Answer\n",
    "    5. Backward Search from Goal to Problem Situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problems\n",
    "\n",
    "- **Optimization Problem**: Finding the best solutions from a set of solutions subject to a set of constraints.\n",
    "\n",
    "\n",
    "#### Types of Optimization Algorithms\n",
    "\n",
    "- **Exact Algorithms**:\n",
    "    - Find Optimal Solution\n",
    "    - High Computational Cost\n",
    "- **Approximate Algorithms**:\n",
    "    - Find Near-Optimal Solution\n",
    "    - Low Computational Cost\n",
    "    \n",
    "#### Approximate Algorithms\n",
    "\n",
    "- **Heuristics**: A solution strategy or rules by trial and error to produce acceptable (optimal or sub-optimal) solutions to complex problems in a reasonably practical time.\n",
    "- **Constructive Methods**: A solution is constructed by iteratively introducing a new component.\n",
    "- **Local Search Methods**: An initial solution is improved by iteratively applying actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal and Problem Formulation\n",
    "\n",
    "- *Requirements for Search: Goal Formulation + Problem Formulation*\n",
    "- *Closed World Assumption: All necessary information about a problem domain is available in each percept so that each state is a complete description of the world.*\n",
    "\n",
    "#### Problem Formulation Template\n",
    "\n",
    "- **State Space**: Complete/Partial Configuration of Problem\n",
    "    - *Required*: Each State = **UNIQUE**\n",
    "- **Initial State**: Beginning Search State\n",
    "- **Goal State**: Ending Search State\n",
    "- **Action Set**: Set of Possible State Transitions\n",
    "- **Cost**: Comparison Function between Solutions\n",
    "\n",
    "#### Extra Terminologies\n",
    "\n",
    "- **State**: Any Possible Agent/Problem Configuration\n",
    "- **Transition Model**: Action Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Search Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Tree Terminology\n",
    "\n",
    "- **Node**: Search Problem State\n",
    "- **Edge**: Search Problem Action\n",
    "- **Fringe**: Frontier/Leaves of Search Tree\n",
    "- **Branching Factor ($b$)**: The maximum number of child nodes extending from a parent node.\n",
    "- **Maximum Depth ($m$)**: The number of edges in the shortest path from the root node to the furthest leaf node.\n",
    "- **Optimal Goal Depth ($d$)**: the number of edges in the shortest path from the root node to an optimal goal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Search Algorithms\n",
    "\n",
    "- **Completeness**: Guarantee Find A Goal Node\n",
    "- **Optimality**: Guarantee Find Best Goal Node\n",
    "- **Time Complexity**: # of Nodes Generated\n",
    "- **Space Complexity**: # of Nodes Stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Search\n",
    "\n",
    "\n",
    "- *Fringe = Queue-Like Data Structure*\n",
    "\n",
    "\n",
    "1. Choose Node\n",
    "2. Test Node\n",
    "3. Expand Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Search Strategies\n",
    "\n",
    "- **Uninformed Strategies**: No knowledge of the direction of goal nodes.\n",
    "    - Breadth-First\n",
    "    - Depth-First\n",
    "    - Depth-Limited\n",
    "    - Uniform-Cost\n",
    "    - Depth-First Iterative Deepening\n",
    "    - Bidirectional\n",
    "- **Informed Strategies**: Domain knowledge of the direction of goal nodes.\n",
    "    - Hill Climbing\n",
    "    - Best-First\n",
    "    - Greedy Search\n",
    "    - Beam Search\n",
    "    - A\n",
    "    - A*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uninformed Search Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breadth-First Search\n",
    "\n",
    "- Expand the shallowest unexpanded nodes, storing the fringe to be expanded in a FIFO queue.\n",
    "\n",
    "#### Properties of Breadth-First Search\n",
    "\n",
    "- *Completeness*:\n",
    "    - Yes - If $b$ is Finite\n",
    "- *Optimality*:\n",
    "    - Yes - If Cost = Depth\n",
    "- *Time Complexity*: $O(b^{d + 1}) \\approx O(b^d)$\n",
    "- *Space Complexity*: $O(b^{d + 1}) \\approx O(b^d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Cost Search\n",
    "\n",
    "- Expand the lowest cost unexpanded node, storing the fringe to be expanded in a minimum priority queue.\n",
    "- **Required**: *No Zero/Negative-Cost Edges*\n",
    "\n",
    "#### Properties of Uniform Cost Search\n",
    "\n",
    "- Let $C^*$ be the path cost to the goal.\n",
    "- Let $\\epsilon$ be the minimum cost of all other actions.\n",
    "- *Completeness*:\n",
    "    - Yes - If $b$ is Finite\n",
    "- *Optimality*: Yes\n",
    "- *Time Complexity*: $O(b^{\\frac{C^*}{\\epsilon} + 1}) \\approx O(b^d)$\n",
    "- *Space Complexity*: $O(b^{\\frac{C^*}{\\epsilon} + 1}) \\approx O(b^d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-First Search\n",
    "\n",
    "- Expand the deepest unexpanded nodes, storing the fringe to be expanded in a LIFO stack.\n",
    "\n",
    "#### Properties of Depth-First Search\n",
    "\n",
    "- *Completeness*:\n",
    "    - No - If Search Space with Infinite-Depth/Loops\n",
    "- *Optimality*: No\n",
    "- *Time Complexity*: $O(b^m)$\n",
    "- *Space Complexity*: $O(bm)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-Limited Search\n",
    "\n",
    "- Execute DFS with a maximum search depth as a restriction.\n",
    "    - Prevents Infinite-Depth Problem\n",
    "    - Prevents Loops Problem\n",
    "\n",
    "#### Properties of Depth-Limited Search\n",
    "\n",
    "- Let $l$ be the maximum search depth.\n",
    "- *Completeness*:\n",
    "    - Yes - If Solution's Depth $d \\le l$\n",
    "- *Optimality*: No\n",
    "- *Time Complexity*: $O(b^l)$\n",
    "- *Space Complexity*: $O(bl)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Deepening Search\n",
    "\n",
    "- Iteratively, execute DLS with an increasing maximum search depth $l$ until a solution is found.\n",
    "\n",
    "#### Properties of Iterative Deepening Search\n",
    "\n",
    "- *Completeness*:\n",
    "    - Yes - If $b$ is Finite\n",
    "- *Optimality*:\n",
    "    - Yes - If Cost = Depth\n",
    "- *Time Complexity*: $O(b^d)$\n",
    "- *Space Complexity*: $O(bd)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breadth-First vs. Depth-First Strategies\n",
    "\n",
    "#### Breadth-First Strategies\n",
    "\n",
    "- High Memory Requirement\n",
    "- Never Stuck on Infinite Depths\n",
    "- Find Shortest Path to Goal\n",
    "\n",
    "#### Depth-First Strategies\n",
    "\n",
    "- Low Memory Requirement\n",
    "- Stuck on Infinite Depths\n",
    "- Find Any Path to Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Repeated States\n",
    "\n",
    "\n",
    "- *Increasing Computational Costs*:\n",
    "\n",
    "\n",
    "1. Do not return to the state your just came from.\n",
    "2. Do not create paths with cycles in them.\n",
    "3. Do not generate any state that was ever created before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- Apply domain knowledge in a problem to search the \"most promising\" branches first.\n",
    "- Potentially, find solutions faster or cheaper than uninformed search algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics\n",
    "\n",
    "- A **heuristic function** $h(n)$ can be used to estimate the \"goodness\" of node $n$.\n",
    "    - $\\forall n, h(n) \\ge 0$\n",
    "    - $h(n) = 0$ $\\implies$ $n$ is a goal node.\n",
    "    - $h(n) = \\infty$ $\\implies$ $n$ is a dead end that does not lead to a goal.\n",
    "- **Admissible/Optimistic**: If a heuristic function never overestimates the cost of reaching the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strong vs. Weak Methods\n",
    "\n",
    "- **Strong Methods**: *Specific Approach to Some Problems*\n",
    "- **Weak Methods**: *General Approach to Many Problems*\n",
    "\n",
    "#### Examples of Weak Methods\n",
    "\n",
    "- **Mean-End Analysis**: A strategy where a representation is formed for the current and goal state, and actions are analyzed that shrink the difference between the two.\n",
    "- **Space Splitting**: A strategy where possible solutions to a problem are listed, and then classes of these solutions are ruled out to shrink the search space.\n",
    "- **Subgoaling**: A strategy where a large problem is split into independent smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best-First Search/Greedy Search\n",
    "\n",
    "- *~Uniform Cost Search with Priority Queue*\n",
    "    - Minimize $f(n) \\mapsto h(n)$\n",
    "    - Greedy If $f(n) = h(n)$\n",
    "\n",
    "#### Properties of Best-First Search/Greedy Search\n",
    "\n",
    "- *Completeness*:\n",
    "    - No - Stuck in Loops\n",
    "- *Optimality*:\n",
    "    - No\n",
    "- *Time Complexity*: $O(b^m)$\n",
    "- *Space Complexity*: $O(b^m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "- *~Breadth-First Search + Reduced Memory Requirements*\n",
    "    - Expands Best $\\beta$ (*Beam Width*) Nodes Per Level\n",
    "\n",
    "#### Properties of Beam Search\n",
    "\n",
    "- *Admissible*:\n",
    "    - No\n",
    "- *Completeness*:\n",
    "    - No\n",
    "- *Optimality*:\n",
    "    - No\n",
    "- *Time Complexity*: $O(\\beta b)$\n",
    "- *Space Complexity*: $O(\\beta b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Search/A&ast; Search\n",
    "\n",
    "- *A Search: Best-First Search with $f(n) = g(n) + h(n)$*\n",
    "    - $g(n)$ is the cost from the start to $n$.\n",
    "    - $h(n)$ is the cost from $n$ to the goal.\n",
    "- *A&ast; Search: Constraint $h(n)\n",
    "\\le h^{\\ast}(n)$*\n",
    "    - $h^{\\ast}(n)$ is the actual minimal path cost from $n$ to the goal.\n",
    "\n",
    "#### Properties of A Search\n",
    "\n",
    "- *Admissible*:\n",
    "    - No\n",
    "- *Completeness*:\n",
    "    - No - If $h(n) \\to \\infty$\n",
    "- *Optimality*:\n",
    "    - No\n",
    "\n",
    "#### Properties of A&ast; Search\n",
    "\n",
    "- *Admissible*:\n",
    "    - Yes - If $h(n) \\le h^{\\ast}(n)$\n",
    "- *Completeness*:\n",
    "    - Yes - If $b$ is finite and only fixed positive costs.\n",
    "- *Optimality*:\n",
    "    - Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing Search\n",
    "\n",
    "\n",
    "- *Improvement of Depth-First Search*\n",
    "- *~Beam Search with $\\beta = 1$*\n",
    "- *~Greedy Search with No Backtracking*\n",
    "- *Not Complete at Local Minima, Plateaus, Ridges*\n",
    "\n",
    "\n",
    "1. Start with an arbitrary solution.\n",
    "2. Attempt to improve the solution by changing a single element at a time.\n",
    "3. Sort the successors of a node according to their heuristic values, and then adding them to the list to be expanded.\n",
    "4. Make changes until no further improvements can be found.\n",
    "\n",
    "#### Rule of Hill Climbing Search\n",
    "\n",
    "- If there is a successor $s$ for node $n$ such that:\n",
    "    - $h(s) < h(n)$ and\n",
    "    - $h(s) \\le h(t)$ for all successors $t$ of $n$.\n",
    "- True $\\implies$ Advance from $n$ to $s$.\n",
    "- False $\\implies$ Halt at $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics\n",
    "\n",
    "- **Perfect Heuristic**: If $h(n) = h^{*}(n)$, then only nodes on the optimal solution are expanded.\n",
    "- **Null Heuristic**: If $h(n) = 0$, then A&ast behaves like uniform cost search.\n",
    "- **Better Heuristic**: If $h_{1}(n) < h_{2}(n) < h^{*}(n)$, then $h_{2}$ is a better heuristic than $h_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Playing as Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- Games involve playing against an opponent, where search problems involve finding a good move, waiting for an opponent's response, and then repeating.\n",
    "- Time is typically limited in each search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation of Games\n",
    "\n",
    "- **Initial State**: *Initial Position + Whose Move It Is*\n",
    "- **Operators**: *Legal Player Moves*\n",
    "- **Goal (Terminal Test)**: *Is Game Over?*\n",
    "- **Utility (Payoff)**: *Measures Outcome/Desirability*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Games\n",
    "\n",
    "- **Perfect Information**: Each player has complete information on the opponent's state and available choices.\n",
    "- **Imperfect Information**: Each player does not have complete information on the opponent's state and available choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Min Strategy\n",
    "\n",
    "- With perfect information and two players, a game tree can be expanded to describe all possible moves of the player and the opponent in the game.\n",
    "- **Zero Sum Games**: *Player Win $\\implies$ Opponent Loss*\n",
    "- **Minimax Principle**: *Minimize the maximum losses that occur.*\n",
    "\n",
    "#### Minimax Algorithm\n",
    "\n",
    "![Example of Minimax Algorithm](images/Minimax_Algorithm_1.png)\n",
    "\n",
    "\n",
    "- **Important Note**: *Bottom-Up*\n",
    "\n",
    "\n",
    "1. Generate the game tree labeling each level with alternating $\\text{MAX}(player)$ and $\\text{MIN}(opponent)$ labels.\n",
    "2. Apply the utility function to each terminal state (leaf) to get its minimax value.\n",
    "3. Extrapolate these minimax values to determine the utility of the nodes on level higher in the search tree.\n",
    "    - For a $\\text{MAX}(player)$ level, select the maximum minimax value of its successors.\n",
    "    - For a $\\text{MIN}(opponent)$ level, select the minimum minimax value of its successors.\n",
    "4. From the root node, select the move which leads to the highest minimax value.\n",
    "\n",
    "#### Limited Depth\n",
    "\n",
    "- For complicated games, a limited depth of the game tree should be explored.\n",
    "- An **evaluation function $f(n)$** is used to measure the \"goodness\" of a game state.\n",
    "\n",
    "#### Properties of Minimax Algorithm\n",
    "\n",
    "- *Completeness*:\n",
    "    - Yes - If game tree is finite\n",
    "- *Optimality*:\n",
    "    - Yes - If opponent is optimal\n",
    "- *Time Complexity*: $O(b^{d})$\n",
    "- *Space Complexity*: $O(bd)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\alpha$-$\\beta$ Pruning\n",
    "\n",
    "- *Branch and Bound: Reduce # of Generated/Evaluated Nodes*\n",
    "    - *Avoid Processing Subtrees $\\ne$ Affecting Result*\n",
    "- **Alpha ($\\alpha$)**: The best value for $\\text{MAX}$ seen so far.\n",
    "    - Used in $\\text{MIN}$ nodes\n",
    "    - Assigned in $\\text{MAX}$ nodes\n",
    "    - Never Decreases\n",
    "- **Beta ($\\beta$)**: The best value for $\\text{MIN}$ seen so far.\n",
    "    - Used in $\\text{MAX}$ nodes\n",
    "    - Assigned in $\\text{MIN}$ nodes\n",
    "    - Never Increases\n",
    "- **Alpha Cutoff** (*Lower Bound*): When the value of a minimum position is less than or equal to the alpha-value of its parent, stop generating further successors.\n",
    "- **Beta Cutoff** (*Upper Bound*): When the value of a maximum position is greater than the beta-value of its parent, stop generating further successors.\n",
    "\n",
    "#### $\\alpha$-$\\beta$ Minimax Algorithm Revisions\n",
    "\n",
    "1. Search discontinued below any $\\text{MIN}$ with $\\beta \\le \\alpha$ of one of its ancestors.\n",
    "    - Set final value of the node to be this $\\beta$ value.\n",
    "2. Search discontinued below any $\\text{MAX}$ with $\\alpha \\ge \\beta$ of one of its ancestors.\n",
    "    - Set final value of the node to be this $\\alpha$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metaheuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "![Overview of Metaheuristic Methods](images/Metaheuristics_1.png)\n",
    "\n",
    "- **Metaheuristics**: High-level heuristics designed to select other heuristics to solve a problem by exploring and exploiting its search space.\n",
    "    - *Approximate Solutions*\n",
    "    - *Nondeterministic Solutions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "\n",
    "- Mechanisms to avoid getting trapped in confined areas of the search space.\n",
    "- Not problem-specific; may use domain-specific knowledge from heuristics controlled by upper-level strategy.\n",
    "- Search history to guide the search.\n",
    "- Hybrid search models where the search identifies neighborhoods where a goal may lie, and then the search is intensified in that area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population-Based Methods\n",
    "\n",
    "- Population-based methods are metaheuristic approaches that apply multiple agents to a search space and can handle multiple simultaneous solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory Methods\n",
    "\n",
    "- Trajectory methods are metaheuristic variants of local search that apply memory structure to avoid getting stuck at local minima, and implement an explorative strategy that tries to avoid revisiting nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Annealing Analogy\n",
    "\n",
    "- Physical annealing involves heating a substance (e.g. a metal) and then letting it cool to increase its ductility and reduce hardness.\n",
    "- The goal is to make the molecules in a cooled substances arrange themselves in a low-energy structure, and the properties of this structure are inuenced by the temperatures reached and the rate of cooling.\n",
    "- A sequence of cooling times and temperatures is referred to as an annealing or cooling schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing Algorithm\n",
    "\n",
    "\n",
    "- Let $s = s_{0}$ be a current solution initialized to $s_{0}$.\n",
    "- Let $t = t_{0}$ be a current temperature initialized to $t_{0}$.\n",
    "- Let $\\alpha$ be a temperature reduction function.\n",
    "\n",
    "\n",
    "1. Repeat,\n",
    "    1. Repeat,\n",
    "        1. Select a solution $s_{i}$ from the neighborhood $N(s)$.\n",
    "        2. Calculate the change in cost $\\Delta C$.\n",
    "        3. If $\\Delta C < 0$, then accept the new solution: $s = s_{i}$.\n",
    "        4. Else, generate a random number $x \\in (0, 1)$.\n",
    "        5. If $x < \\exp(\\frac{-\\Delta C}{t})$, then accept the new solution: $s = s_{i}$.\n",
    "    2. Until maximum number of iteration for $t$.\n",
    "    3. Decrease $t$ using $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "![Simulated Annealing Strategy](images/Simulated_Annealing_1.png)\n",
    "\n",
    "- Simulated annealing always accepts better solutions.\n",
    "- Simulated annealing randomly accepts worse solutions.\n",
    "- At higher temperatures, explore parameter space.\n",
    "- At lower temperatures, restrict exploration.\n",
    "\n",
    "$$\\text{ Low Temperature } \\wedge \\text{ High Change in Cost } \\implies \\text{ Low Acceptance Probability }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annealing Schedule\n",
    "\n",
    "- **Annealing Schedule**: *Adjusts Temperature*\n",
    "    - *Initial Temperature*\n",
    "    - *Final Temperature*\n",
    "    - *Temperature Decrement Rule*\n",
    "    - *Temperature Iterations*\n",
    "    \n",
    "#### Initial Temperature\n",
    "\n",
    "- The initial temperature should be high enough to allow exploration to any part of the search space.\n",
    "- If the initial temperature is too hot, simulated annealing would behave too randomly.\n",
    "- The maximum change of a cost function should be considered when setting the initial temperature.\n",
    "- **General Rule**: Set the initial temperature to accept around $60\\%$ of worse solutions.\n",
    "\n",
    "#### Final Temperature\n",
    "\n",
    "- The final temperature should be quite low but not neccessarily have to reach zero.\n",
    "- A search using simulated annealing can be stopped once no better moves are being found and no worse moves are being accepted.\n",
    "\n",
    "#### Temperature Decrement Rule\n",
    "\n",
    "- **Linear**: $t = t - \\alpha$\n",
    "- **Geometric**: $t = t \\times alpha$\n",
    "- **Slow Decrease**: $t = \\frac{t}{1 + \\beta t}$\n",
    "\n",
    "#### Temperature Iterations\n",
    "\n",
    "- Enough iterations should be allowed at every temperature for the system to be stable at that temperature.\n",
    "- If the search space is very large, a large number of iterations may be required.\n",
    "- If the slow decrease rule is used, one iteration per temperature should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "\n",
    "- Simulated annealing is guaranteed to eventually converge to a solution at a constant temperature, assuming some sequence of moves leads to the goal state.\n",
    "- When temperature is not constant, convergence can still be guaranteed but only under conditions that result in very slow temperature reduction and an exponential increase in the number of iterations at each temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Easy\n",
    "- Widely Applicable\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "- Time Complexity\n",
    "- Many Tunable Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation\n",
    "\n",
    "- **Adaptation** refers to adapting the critical parameters of the simulated annealing algorithm.\n",
    "\n",
    "#### Initial Temperature\n",
    "\n",
    "- Finding the right temperature is very problem-specific, and different search algorithms can be applied in finding this temperature.\n",
    "\n",
    "#### Cooling Schedule\n",
    "\n",
    "- Some cooling schedules require that only the cooling rate $\\alpha$ is specified, and the remainder of parameters are automatically determined using a linear random combination of previously accepted states and parameters to estimate new steps and parameters.\n",
    "\n",
    "#### Probability of Acceptance\n",
    "\n",
    "- Some attempted adaptations concerning the probability of acceptance include using a lookup table for the relevant calculations (to decrease computation time) or using a different, non-exponential probability formula.\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "- Cost functions that return similar values for many different states tend to not lead to effective search.\n",
    "- As an alternative, a cost function can have a penalty term associated with certain types of states, and weighting of these penalty terms can vary dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooperation\n",
    "\n",
    "- Cooperative simulated annealing involves multiple concurrent runs of a simulated annealing search algorithm on a search space.\n",
    "- Potential solutions are produced by somehow combining the value of a run with the value of a random previous run of the algorithm.\n",
    "- This exchange of information from other solutions is known as cooperative transition, and is a concept borrowed from genetic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabu Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- **Tabu Search**: A general trajectory-based metaheuristic strategy for controlling inner heuristics.\n",
    "    - *Combination of Local Search Strategy + Search Experience Model $\\implies$ Escape Local Minima + Explorative Strategy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Search Strategy\n",
    "\n",
    "1. Start with an initial feasible solution.\n",
    "2. Repeat,\n",
    "    1. Generate a neighboring solution by applying a series of local modifications.\n",
    "    2. If the new solution is better, then replace the current solution.\n",
    "    \n",
    "#### Local Search Strategy Challenges\n",
    "\n",
    "1. It can be costly to consider all possible local modifications.\n",
    "2. It can get stuck in local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ideas\n",
    "\n",
    "- Penalize moves that take a solution back to a previously visited (tabu) state.\n",
    "- Accepts non-improving solutions in order to escape from local optima and eventually find a better solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of Memory\n",
    "\n",
    "- **Short-Term Memory** (*Recency of Occurrence*): Prevent the search from revisiting recently visited solutions.\n",
    "    - **Tabu List**: A short-term memory structure that stores recent moves applied to the current solutions or their attributes.\n",
    "    - **Tabu Tenure**: The number of iterations $T$ for which a certain move or its attributes are kept in the list.\n",
    "    - Complete solutions are rarely used because of the space requirement.\n",
    "- **Long-Term Memory** (*Frequency of Occurrence*): Prevent the search from revisiting frequently visited solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood\n",
    "\n",
    "- When selecting a new state, consider neighbors that are not on the tabu list.\n",
    "$$N(s) - T(s)$$\n",
    "- The neighborhood structure $N(s)$ can be reduced and modified based on history and knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termination Conditions\n",
    "\n",
    "- *Sample Conditions*:\n",
    "    - No feasible solution in the neighborhood of current solution.\n",
    "    - Reached the maximum number of iterations allowed.\n",
    "    - The number of iterations since the last improvement is larger than a specified number.\n",
    "    - Evidence shows that an optimum solution has been obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates and Aspiration\n",
    "\n",
    "- A **candidate list** stores the potential solutions in a neighborhood to be examined.\n",
    "    - Isolate regions of a neighborhood with desirable features, aiming to find a solution more efficiently.\n",
    "    - At times, it can be desirable to include a move in a candidate list even if it is tabu in order to prevent **stagnation**.\n",
    "    - **Aspiration Criteria**: *Approaches for Canceling Tabus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabu Search Algorithm\n",
    "\n",
    "\n",
    "- Let $s = s_{0}$ be a current solution initialized to $s_{0}$.\n",
    "- Let $N(s)$ be the neighborhood of the current solution.\n",
    "- Let $T(s)$ be the tabu list of the current solution.\n",
    "- Let $A(s)$ be the aspiration list of the current solution.\n",
    "\n",
    "\n",
    "1. Repeat,\n",
    "    1. Select the best solution $s'$ from $N^{\\ast}(s) = N(s) - T(s) + A(s)$.\n",
    "    2. Memorize $s'$ if it is improves the best known solution.\n",
    "    3. Let $s = s'$.\n",
    "    4. Update $T(s)$ and $A(s)$\n",
    "2. Until termination criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Tabu Restrictions\n",
    "\n",
    "- *Sample Restrictions*:\n",
    "    - Not picking a move that involves the same exchange of positions of a tabu move.\n",
    "    - Not picking a move that results in positions that previously appeared in a tabu move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Tabu Tenure\n",
    "\n",
    "- *Sample Strategies*:\n",
    "    - Statically assigning $T$ to be a constant: $\\sqrt{n}$ where $n$ is the problem size.\n",
    "    - Dynamically letting $T$ vary between a $T_{min}$ and a $T_{max}$.\n",
    "        - *Advantages: Better Limits Cycles*\n",
    "        - *Disadvantages: May Contain Cycles Longer Than Tabu Tenure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Aspiration Criteria\n",
    "\n",
    "- *Sample Strategies*:\n",
    "    - By default, where a tabu move becomes admissible if it yields a better solution than any found so far.\n",
    "    - By objective, where a tabu move becomes admissible if it yield a solution better than an aspiration value.\n",
    "    - By search direction, where a tabu move becomes admissible if the direction of the search remains constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensification\n",
    "\n",
    "- **Intensification**: The process of exploiting a small portion of the search space (e.g. penalizing solutions far from the current solution).\n",
    "- *General Idea*: Locally optimize a best known solution while trying to preserve the general components of that solution (based on short-term memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversification\n",
    "\n",
    "- **Diversification**: The process of forcing the search into unexplored areas (e.g. penalizing solutions close to the current solution).\n",
    "- *General Problem*: Tabu search can miss some good solutions in unexplored search space areas.\n",
    "\n",
    "#### Diversification Strategies\n",
    "\n",
    "- **Restart Diversification**: Where components rarely appearing in solutions are forced into new solutions.\n",
    "- **Continuous Diversification**: Where the evaluation of possible moves is biased by a term related to component frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation\n",
    "\n",
    "- **Adaptation** refers to a series of techniques for varying the tabu tenure.\n",
    "    - If the tabu tenure is too small, then cycles in the search are likely.\n",
    "    - If the tabu tenure is too big, then many moves could be prevented at each iteration.\n",
    "\n",
    "#### Adaptation Techniques\n",
    "\n",
    "- Randomly select a new tenure from a pre-computed range every predetermined number of iterations.\n",
    "- Set the tenure to one if a best-so-far solution is found, decreasing it in an improving phase, and increasing it in a worsening phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooperation\n",
    "\n",
    "- Cooperative tabu search involves multiple concurrent runs of a tabu search algorithm on a search space.\n",
    "- **Synchronous Communication**: Search agents exchange information every fixed number of iterations.\n",
    "- **Asynchronous Communication**: Search agents relay their best-so-far results to a central memory.\n",
    "- **Forced Diversification**: A search agent can replace its own best-so-far solution with an incoming solution.\n",
    "- **Conditional Import**: A seach agent can replace its own best-so-far solution with an incoming solution only if the incoming solution is better.\n",
    "\n",
    "#### Observations on Tabu Search Cooperation\n",
    "\n",
    "- Increasing the number of search agents improves the solution up to a certain point.\n",
    "- Increasing the number of synchronization messages increases the computation time due to message passing overhead.\n",
    "- Conditional imports are almost always preferable to forced diversification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- **Swarm Intelligence**: The collective behavior of decentralized, self-organized systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions\n",
    "\n",
    "- **Swarm**: A group of agents that communicate with each other by acting on their local environment.\n",
    "- Complex problem-solving behavior may emerge not as a result of any particular individual, but rather as a result of their interactions.\n",
    "- Interactions between individuals may be direct (physical contact) or indirect (via local change to the environment, **stigmergy**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "\n",
    "- **Flexibility**: System performance is adaptive to internal or external changes.\n",
    "- **Robustness**: System can perform even if some individuals fail.\n",
    "- **Decentralization**: Control is distributed among individuals rather than allocated to some master.\n",
    "- **Self-Organization**: Global behaviors emerge as a result of local interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models of Behavior\n",
    "\n",
    "- **Swarm**: Group with little parallel alignment.\n",
    "- **Torus**: Group in which individuals rotate around an empty core in one direction.\n",
    "- **Dynamic Parallel Group**: Group where individuals are polarized and move as a coherent group, but can still move throughout the group such that the group density and form fluctuates.\n",
    "- **Highly Parallel Group**: Group similar to dynamical parallel group but with minimal fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swarm Intelligence Problem Solving\n",
    "\n",
    "- **Proximity**: The swarm should be able to carry out simple space and time computations.\n",
    "- **Quality**: The swarm should be able to respond to quality factors in its environment.\n",
    "- **Diverse Response**: The swarm should not commit to excessively narrow channels of exploration.\n",
    "- **Stability**: The swarm should not rapidly alter its behavior in response to all environmental changes.\n",
    "- **Adaptability**: The swarm should be willing to change its behavior when it is worth the computational price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ant Colony Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "- **Ant Colony Optimization**: A search technique based on the swarm intelligence of ants.\n",
    "- Ants interact with one another through stigmergy, meaning that individual ants can make changes to their environment to be picked up by other ants.\n",
    "- Ants do this by forming trails with substances known as pheromone. Ants tend to follow paths with higher pheromone concentrations. Pheromone also evaporates over time, so more recent pheromone deposits have a greater influence than older ones.\n",
    "- Ants can use this mechanism to find the shortest path to a destination. If multiple paths are available, ants will pick their initial paths randomly. Ants that take the shorter path will return faster, so their trail will have more unevaporated pheromone. This makes other ants more likely to pick this trail, and eventually, they converge on this shortest path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACO Algorithm\n",
    "\n",
    "\n",
    "- Let $G = (N, E)$ be a graph where $N$ is a set of nodes and $E$ is a set of edges.\n",
    "- Let $d_{i, j}$ be the length of each edge $(i, j)$.\n",
    "- Let $\\tau_{i, j}$ be the amount of pheromones of each edge $(i, j)$.\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "- Initialize all edges with a small amount of pheromones.\n",
    "- Initialize the source node with a group of $m$ ants.\n",
    "\n",
    "#### Transition Rule\n",
    "\n",
    "- At each node $i$, with adjacent nodes $N_{i}$, an ant can move to an adjacent node $j$ with the following probability.\n",
    "$$\\frac{\\frac{\\tau_{i, j}^{\\alpha}}{d_{i, j}^{\\beta}}}{\\sum_{n \\in N_{i}} \\frac{\\tau_{i, n}^{\\alpha}}{d_{i, n}^{\\beta}}}$$\n",
    "- $\\alpha$ and $\\beta$ balance the local and the global search abilities respectively.\n",
    "\n",
    "#### Pheromone Evaporation and Update\n",
    "\n",
    "- In each step, the amount of pheromone on the trail is evaporated (i.e. $\\tau = \\tau \\cdot (1 - p)$).\n",
    "- In each step, the amount of pheromone on the trail is increased if ants choose the trail (i.e. $\\tau = \\tau + \\Delta\\tau$).\n",
    "    - **Ant Density Model**: Where a constant value is added.\n",
    "    - **Ant Quality Model**: Where a constant is divided by the edge length.\n",
    "    - **Online Delayed Model**: Where an ant first builds a solution, then traces its path backwards and adds pheromone based on the solution quality.\n",
    "    \n",
    "#### Termination Criteria\n",
    "\n",
    "- *Sample Criteria*:\n",
    "    - *Maximum Number of Iterations Reached*\n",
    "    - *Good Enough Solution Reached*\n",
    "    - *Stagnation Occurs*\n",
    "    \n",
    "#### Tunable Parameters\n",
    "\n",
    "- *Number of Ants*\n",
    "- *Maximum Number of Iterations*\n",
    "- *Initial Pheromone*\n",
    "- *Pheromone Delay Parameters (i.e. $p$)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ant Colony System Algorithm\n",
    "\n",
    "- The **ant colony system** (ACS) algorithm features the following extensions on ACO:\n",
    "    - The transition rules sometimes just choose the best path (i.e. acts greedily) instead of applying probabilistic selection.\n",
    "    - Pheromone update is only based on the best solution (i.e. highest increase in $\\tau$ either globally or in iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Min Ant System Algorithm\n",
    "\n",
    "- The **max-min ant system** algorithm is an extension that restricts pheromone values within a range.\n",
    "    - The max and min pheromone values can be adjusted to favor exploration over exploitation during early phases of the search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation\n",
    "\n",
    "#### ACSGA-TSP\n",
    "\n",
    "- Have a genetic algorithm running on top of ACS to attempt to optimize its parameter values.\n",
    "- Each ant has certain parameters (e.g. $p$, $\\beta$) encoded into a chromosome.\n",
    "- New generations are formed by crossing over the best-performing ants.\n",
    "\n",
    "#### Near Parameter Free ACS\n",
    "\n",
    "- Apply an ant approach to optimize ant parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooperation\n",
    "\n",
    "- Heterogeneous cooperation approaches involve ants in different colonies having different behavior.\n",
    "    - This can be used, for instance, to optimize for different criteria of a solution.\n",
    "- Homoegeneous cooperation approaches involve ants in different colonies having similar behavior.\n",
    "    - Exchange of information can take place in a similar way to that of genetic algorithms.\n",
    "    - Homogeneous cooperation implementations can be course-grained, where each process holds a single ant, or fine-grained, where each process holds a colony.\n",
    "    - An effective version of homogeneous cooperation is a circular exchange of locally best solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Memory of entire colony retained.\n",
    "- Poor solutions rarely converged on due to many combinations of path selection.\n",
    "- Effective handling of dynamic environments.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "- Theoretical analysis is limited.\n",
    "- Many parameters to tune.\n",
    "- Convergence may take long."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
