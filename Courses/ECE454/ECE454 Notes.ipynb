{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 454 - Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed System\n",
    "\n",
    "- **Distributed System**: A collection of autonomous computing elements that appears to its users as a single coherent system.\n",
    "\n",
    "#### Motivations for Distributed Systems\n",
    "\n",
    "1. Resource Sharing\n",
    "2. Simplify Processes by Integrating Multiple Systems\n",
    "3. Limitations in Centralized Systems: Weak/Unreliable\n",
    "4. Distributed/Mobile Users\n",
    "\n",
    "#### Goals for Distributed Systems\n",
    "\n",
    "1. Resource Sharing\n",
    "    - CPUs, Data, Peripherals, Storage.\n",
    "2. Transparency\n",
    "    - Access, Location, Migration, Relocation, Replication, Concurrency, Failure.\n",
    "3. Open\n",
    "    - Interoperability, Composability, Extensibility.\n",
    "4. Scalable\n",
    "    - Size, Geography, Administration.\n",
    "    \n",
    "#### Types of Distributed Systems\n",
    "\n",
    "- Web Services\n",
    "- High Performance Computing, Cluster Computing, Cloud Computing, Grid Computing\n",
    "- Transaction Processing\n",
    "- Enterprise Application Integration\n",
    "- Internet of Things, Sensor Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware\n",
    "\n",
    "- **Middleware**: A layer of software that separates applications from the underlying platforms.\n",
    "    - Supports Heterogeneous Computers/Networks.\n",
    "    - *e.g.*: Communication, Transactions, Service Composition, Reliability.\n",
    "    - **Single-System View**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Techniques\n",
    "\n",
    "1. *Hiding Communication Latencies*: At Server vs. At Client?\n",
    "2. *Partitioning*\n",
    "3. *Replication*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallacies of Networked and Distributed Computing\n",
    "\n",
    "1. Network is reliable.\n",
    "2. Network is secure.\n",
    "3. Network is homogeneous.\n",
    "4. Topology is static.\n",
    "5. Latency is zero.\n",
    "6. Bandwidth is infinite.\n",
    "7. Transport cost is zero.\n",
    "8. There is only one administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory vs. Message Passing\n",
    "\n",
    "- **Shared Memory**:\n",
    "    - Less Scalable\n",
    "    - Faster\n",
    "    - CPU-Intensive Problems\n",
    "    - Parallel Computing\n",
    "- **Message Passing**:\n",
    "    - More Scalable\n",
    "    - Slower\n",
    "    - Resource Sharing / Coordination Problems\n",
    "    - Distributed Computing\n",
    "- Apache Hadoop is an example of a hybrid computing framework that uses message passing at a broad-view and shared memory at a detailed-view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud and Grid Computing\n",
    "\n",
    "- **IaaS**: Infrastructure as a Service\n",
    "    - VM Computation, Block File Storage\n",
    "- **PaaS**: Platform as a Service\n",
    "    - Software Frameworks, Databases\n",
    "- **SaaS**: Software as a Service\n",
    "    - Web Services, Business Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Processing Systems\n",
    "\n",
    "- **Transaction Processing Monitor**: Coordinates Distributed Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "- **Component**: A modular unit with well-defined interfaces.\n",
    "- **Connector**: A mechanism that mediates communication, coordination, or cooperation among components.\n",
    "- **Software Architecture**: Organization of software components.\n",
    "- **System Architecture**: Instantiation of software architecture in which software components are placed on real machines.\n",
    "- **Autonomic System**: Adapts to its environment by monitoring its own behavior and reacting accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectural Styles\n",
    "\n",
    "- Layered\n",
    "    - *Note: Assignment Topic*\n",
    "- Object-Based\n",
    "- Data-Centered\n",
    "- Event-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Architecture\n",
    "\n",
    "![Layers](images/Architecture_1.png)\n",
    "\n",
    "- **Examples**:\n",
    "    - Database Server, Application Server, Client\n",
    "    - SSH Server, SSH Client\n",
    "- Requests Flow Down Stack\n",
    "- Responses Flow Up Stack\n",
    "- *Handle-Upcall*: Async Notification\n",
    "    - Subscribe with Handle\n",
    "    - Publish with Upcall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client-Server Interactions\n",
    "\n",
    "![Client-Server Interactions](images/Architecture_2.png)\n",
    "\n",
    "- *Bolded Lines* = Busy\n",
    "- *Dashed Lines* = Idle\n",
    "- **Client**: Initiates with a Request\n",
    "- **Server**: Follows with a Response\n",
    "- **Total Round-Trip Time**: $(N - 1) \\times t_{\\text{Request-Response}}$\n",
    "    - Layering can reduce the amount of processing time per layer, but the additional communication overhead between the layers introduces diminishing returns.\n",
    "- An intermediate layer can be both a client and a server to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Tiered Architecture\n",
    "\n",
    "- Logical Software Layers $\\mapsto$ Physical Tiers\n",
    "    - *Trade-Offs*: Ease of Maintenance vs. Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal vs. Vertical Distribution\n",
    "\n",
    "- **Vertical Distribution**: When the logical layers of a system are organized as separate physical tiers.\n",
    "    - *Performance*: High.\n",
    "    - *Scalability*: Low.\n",
    "    - *Dependability*: Low-Medium.\n",
    "- **Horizontal Distribution**: When one logical layer is split across multiple machines - **sharding**.\n",
    "    - *Performance*: Low.\n",
    "    - *Scalability*: High.\n",
    "    - *Dependability*: Medium-High."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object-Based Architecture\n",
    "\n",
    "- In an object-based architecture, components communicate using remote object references and method calls.\n",
    "\n",
    "#### Problems with Object-Based Architecture\n",
    "\n",
    "- Complex Communication Interfaces\n",
    "- Complex Communication Costs\n",
    "- Not Scalable\n",
    "- Not Language Agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Centered Architecture\n",
    "\n",
    "- In a data-centered architecture, components communicate by accessing a shared data repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-Based Architecture\n",
    "\n",
    "![Publish/Subscribe Middleware](images/Architecture_3.png)\n",
    "\n",
    "- In an event-based architecture, components communicate by propagating events using a publish/subscribe system.\n",
    "\n",
    "#### Handling Asynchronous Delivery Failure\n",
    "\n",
    "- *At-Least Once Delivery*: Do Retransmit\n",
    "- *At-Most Once Delivery*: Do Not Retransmit\n",
    "- *Exactly Once Delivery*: Unknown/Unachievable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer-to-Peer Systems\n",
    "\n",
    "![Chord's Finger Table](images/Architecture_4.png)\n",
    "\n",
    "- In a peer-to-peer system, decentralized processes are organized in an overlay network that defines a set of communication channels.\n",
    "- In a peer-to-peer, distributed hash table, a keyspace is represented by a consistent hash ring on top of which nodes partition ranges amongst themselves.\n",
    "- The mappings of partition ranges to nodes are maintained by a finger table which can be queried in a logarithm process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Architectures\n",
    "\n",
    "- BitTorrent is an example of a hybrid architecture combining a client-server architecture and a peer-to-peer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Management\n",
    "\n",
    "![Self-Management Systems](images/Architecture_5.png)\n",
    "\n",
    "- In self-management, systems use a feedback control loop that monitors system behaviors and adjusts system operations.\n",
    "- **Assignment Note**: Useful for Unknown Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPC\n",
    "\n",
    "- **Inter-Process Communication (IPC)**: Expensive b/c Context Switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n",
    "\n",
    "- Typically, an operating system kernel support multi-threading through **lightweight processes (LWP)**.\n",
    "- **Assignment Note**: Do Not Spawn Too Many Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Threaded Servers\n",
    "\n",
    "- **Dispatcher/Worker Design**: A dispatcher thread receives requests from the network and feeds them to a pool of worker threads.\n",
    "- **Assignment Note**: Useful for Assignment 1 & Partition into Sequential Work and Parallel Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware and Software Interfaces\n",
    "\n",
    "![Hardware and Software Interfaces](images/Processes_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtualization\n",
    "\n",
    "![VMs](images/Processes_2.png)\n",
    "\n",
    "- **Advantage**:\n",
    "    - Portability\n",
    "    - Live Migration of VMs\n",
    "    - Replication for Availability/Fault Tolerance\n",
    "- **Disadvantage**:\n",
    "    - Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Clusters\n",
    "\n",
    "![Three Physical Tier](images/Processes_3.png)\n",
    "\n",
    "- **Assignment Note**: Useful for Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Network Model\n",
    "\n",
    "![Layered Network Model](images/Communication_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Procedure Calls\n",
    "\n",
    "- **Remote Procedure Calls**: A transient communication abstraction implemented using a client-server protocol.\n",
    "- **Client Stub**: Translate a RPC on the client.\n",
    "- **Server Stub**: Translate a RPC on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of a RPC\n",
    "\n",
    "![Steps of a RPC](images/Communication_2.png)\n",
    "\n",
    "\n",
    "1. The client process invokes the client stub using an ordinary procedure call.\n",
    "2. The client stub builds a message and passes it to the client's OS.\n",
    "3. The client's OS sends the message to the server's OS.\n",
    "4. The server's OS delivers the message to the server stub.\n",
    "5. The server stub unpacks the parameters and invokes the appropriate service handler in the server process.\n",
    "6. The **service handler** does the work and returns the result to the server stub.\n",
    "7. The server stub packs the result into a message and passes it to the server's OS.\n",
    "8. The server's OS sends the message to the client's OS.\n",
    "9. The client's OS delivers the message to the client stub.\n",
    "10. The client stub unpacks the result and returns it to the client process.\n",
    "\n",
    "\n",
    "- **Parameter Marshalling**: Packing Parameter $\\to$ Message\n",
    "    - Processor Architectures, Network Protocols, and VMs $\\implies$ **Little-Endian** vs. **Big-Endian**\n",
    "- **Number of System Calls**: 4\n",
    "    1. Client Process $\\to$ Client OS Socket\n",
    "    2. Server OS Socket $\\to$ Server Process\n",
    "    3. Server Process $\\to$ Server OS Socket\n",
    "    4. Client OS Socket $\\to$ Client Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining RPC Interfaces\n",
    "\n",
    "- **Interface Definition Language (IDL)**: Specify RPC Signatures $\\to$ Client/Server Stubs\n",
    "    - High-Level Format\n",
    "    - Parameter Ordering\n",
    "    - Byte Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous vs. Asynchronous RPCs\n",
    "\n",
    "- **Synchronous RPC**: The client blocks to wait for the return value.\n",
    "- **Asynchronous RPC**: The client blocks to wait for the server acknowledgement of the receipt of the request.\n",
    "- **One-Way RPC**: The client does not block to wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Queuing Model\n",
    "\n",
    "![Message Queue Interface](images/Communication_3.png)\n",
    "\n",
    "- **Message Queue**: Alternative to RPCs\n",
    "- **Persistent Communication**: Loose Coupling between Client/Server\n",
    "    - *Advantage*: Resilient to Client/Server Hardware Failure\n",
    "    - *Disadvantage*: Guaranteed Delivery = Impossible\n",
    "- **Message-Oriented Middleware (MOM)**: Asynchronous Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Coupling\n",
    "\n",
    "- **Referential Coupling**: When one process explicitly references another.\n",
    "    - *Positive Example*: RPC client connects to server using an IP address and a port number\n",
    "    - *Negative Example*: Publisher inserts a news item into a pub-sub system without knowing which subscriber will read it.\n",
    "- **Temporal Coupling**: Communicating processes must both be up and running.\n",
    "    - *Positive Example*: A client cannot execute a RPC if the server is down.\n",
    "    - *Negative Example*: A producer appends a job to a message queue today, and a consumer extracts the job tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPC vs. MOM\n",
    "\n",
    "#### RPC\n",
    "\n",
    "- Used mostly for two-way communication, particularly where the client requires immediate response from the server.\n",
    "- The middleware is linked into the client and the server processes.\n",
    "- Tighter coupling means that server failure can prevent client from making progress.\n",
    "\n",
    "#### MOM\n",
    "\n",
    "- Used mostly for one-way communication where one party does not require an immediate response from another.\n",
    "- The middleware is a separate component between the sender/publisher/producer and the receiver/subscriber/consumer.\n",
    "- Looser coupling isolates one process from another which contributes to flexibility and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed File Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Remote Files\n",
    "\n",
    "![DFS Models](images/DFS_1.png)\n",
    "\n",
    "- **Remote Access Model**\n",
    "- **Upload/Download Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network File System (NFS)\n",
    "\n",
    "![Overview of NFS](images/DFS_2.png)\n",
    "\n",
    "- *Supports Client-Side Caching*\n",
    "    - Modifications are flushed to the server when the client closes the file.\n",
    "    - Consistency is implementation dependent.\n",
    "\n",
    "![Authority Delegation](images/DFS_3.png)\n",
    "    \n",
    "- *Supports Authority Delegation*\n",
    "    - A server can delegate authority to a client and recall it through a callback mechanism.\n",
    "    \n",
    "![Compound Procedure](images/DFS_4.png)\n",
    "    \n",
    "- *Supports Compound Procedures*\n",
    "    - Multiple Round Trips to Single Round Trip\n",
    "\n",
    "![Partial Exports](images/DFS_5.png)\n",
    "\n",
    "- *Supports Partial Exports*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google File System (GFS)\n",
    "\n",
    "![Google File System](images/DFS_6.png)\n",
    "\n",
    "- **GFS**: A distributed file system that stripes files across inexpensive commodity servers without RAID.\n",
    "    - *Layered Above Linux File System*\n",
    "    - *Fault Tolerance Through Software*\n",
    "- **GFS Master**: *Stores Metadata About Files/Chunks*\n",
    "    - *Metadata Cache in Main Memory*\n",
    "    - *Updated Log in Local Storage*\n",
    "    - *Periodically Polls Client Servers for Consistency*\n",
    "    \n",
    "#### Reading a File\n",
    "\n",
    "1. A client sends the file name and chunk index to the master.\n",
    "2. The master responds with a contact address.\n",
    "3. The client then pulls data directly from a chunk server, bypassing the master.\n",
    "\n",
    "#### Updating a File\n",
    "\n",
    "1. The client pushes its updates to the nearest chunk server holding the data.\n",
    "2. The nearest chunk server pushes the update to the next closest chunk server holding the data, and so on.\n",
    "3. When all replicas have received the data, the primary chunk server assigns a sequence number to the update operation and passes it on to the secondary chunk servers.\n",
    "4. The primary replica informs the client that the update is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Sharing Semantics\n",
    "\n",
    "![File Sharing Semantics](images/DFS_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Architecture\n",
    "\n",
    "![Hadoop High-Level Architecture](images/Hadoop_1.png)\n",
    "\n",
    "- Transform lists of input data elements into lists of output data elements by applying *Mappers* and *Reducers*\n",
    "    - *Immutable Data*\n",
    "    - *No Communication*\n",
    "    \n",
    "#### Mapper\n",
    "\n",
    "- A list of input data elements are iterated and individually transformed into zero or more output data elements.\n",
    "\n",
    "#### Reducer\n",
    "\n",
    "- A list of input data elements are iterated and individually aggregated into a single output data element.\n",
    "\n",
    "#### Combiner\n",
    "\n",
    "- An optional component that consumes the outputs of a mapper to produce a summary as the inputs for a reducer.\n",
    "\n",
    "#### Terms\n",
    "\n",
    "- **InputSplit**: A unit of work assigned to one map task.\n",
    "    - Usually corresponds to a chunk of an input file.\n",
    "    - Each record in a file belongs to exactly one input split and the framework takes care of dealing with record boundaries.\n",
    "- **InputFormat**: Determines how the input files are parsed, and defines the input splits.\n",
    "- **OutputFormat**: Determines how the output files are formatted.\n",
    "- **RecordReader**: Reads data from an input split and creates key-value pairs for the mapper.\n",
    "- **RecordWriter**: Writes key-value pairs to output files.\n",
    "- **Partitioner**: Determines which partition a given key-value pair will go to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow\n",
    "\n",
    "![MapReduce Data Flow 1](images/Hadoop_2.png)\n",
    "\n",
    "![MapReduce Data Flow 2](images/Hadoop_3.png)\n",
    "\n",
    "- **Shuffle**: The process of partitioning by reducer, sorting and copying data partitions from mappers to reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance\n",
    "\n",
    "- *Primarily: Restart Failed Tasks*\n",
    "    1. Individual *TaskTrackers* periodically emit a heartbeat to the *JobTracker*.\n",
    "    2. If a *TaskTracker* fails to emit a heartbeat to the *JobTracker*, the *JobTracker* assumes that the *TaskTracker* crashed.\n",
    "    3. If the failed node was mapping, then other *TaskTrackers* will be asked to re-execute all the map tasks previously run by the failed *TaskTracker*.\n",
    "    - *Must be Side-Effect Free*\n",
    "    4. If the failed node was reducing, then other *TaskTrackers* will be asked to re-execute all reduce tasks that were in progress on the failed *TaskTracker*.\n",
    "    - *Must be Side-Effect Free*\n",
    "- *Secondarily: Speculative Execution*\n",
    "    - If some **straggler** nodes rate limit the rest of the program, Hadoop will schedule redundant copies of remaining tasks across several nodes which do not have other work to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Design Patterns\n",
    "\n",
    "#### Counts and Summations\n",
    "\n",
    "- A mapper can emit a tuple of an element and one for each element.\n",
    "- A mapper can aggregate the counts for each element and emit a tuple of the element and its count.\n",
    "- A combiner can aggregate the counts across all the elements processed by a mapper.\n",
    "\n",
    "#### Selection\n",
    "\n",
    "- A mapper can emit a tuple for each element that satisfies a predicate.\n",
    "\n",
    "#### Projection\n",
    "\n",
    "- A mapper can emit a tuple whose fields are a subset of each element.\n",
    "- A reducer can eliminate duplicates.\n",
    "\n",
    "#### Inverted Index\n",
    "\n",
    "- A mapper can emit a tuple of a value and a key in that specific order.\n",
    "- A reducer can aggregate all the keys for a distinct value.\n",
    "\n",
    "#### Cross-Correlation\n",
    "\n",
    "- **Problem**: Given a set of tuples of items, for each possible pair of items, calculate the number of tuples where these items co-occur.\n",
    "\n",
    "##### Pairs Approach (Slow)\n",
    "\n",
    "```\n",
    "class Mapper\n",
    "  method Map(void, items [i1, i2, ...])\n",
    "    for all item i in [i1, i2, ...]\n",
    "      for all item j in [i1, i2, ...] such that j > i\n",
    "        Emit(pair [i, j], count 1)\n",
    "\n",
    "class Reducer\n",
    "  method Reduce(pair [i, j], counts [c1, c2, ...])\n",
    "    s = sum([c1, c2, ...])\n",
    "    Emit(pair [i, j], count s)\n",
    "```\n",
    "\n",
    "##### Stripes Approach (Fast)\n",
    "\n",
    "```\n",
    "class Mapper\n",
    "  method Map(void, items [i1, i2, ...])\n",
    "    for all item i in [i1, i2, ...]\n",
    "      H = new AssociativeArray : item -> counter\n",
    "      for all item j in [i1, i2, ...] such that j > i\n",
    "        H{j} = H{j} + 1\n",
    "      Emit(item i, stripe H)\n",
    "      \n",
    "class Reducer\n",
    "  method Reduce(item i, stripes [H1, H2, ...])\n",
    "    H = new AssociativeArray : item -> counter\n",
    "    H = merge-sum([H1, H2, ...])\n",
    "    for all item j in H.keys()\n",
    "      Emit(pair [i, j], H{j})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "- **RDDs** are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage\n",
    "\n",
    "![Lineage](images/Spark_1.png)\n",
    "\n",
    "- A **lineage** is a directed acyclic graph that expresses the dependencies between RDDs such that a RDD can be rebuilt in the event of a failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and Actions\n",
    "\n",
    "- **Transformations**: Operations that convert one RDDs or a pair of RDDs into another RDD.\n",
    "- **Actions**: Operations that convert a RDD into an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow vs. Wide Dependencies\n",
    "\n",
    "![Dependencies](images/Spark_2.png)\n",
    "\n",
    "![Execution](images/Spark_3.png)\n",
    "\n",
    "- When an action is invoked on a RDD, the Spark scheduler examines the lineage graph of the RDD and builds a directed acyclic graph of transformations.\n",
    "- The transformations in the DAG are grouped into **stages**.\n",
    "- A **stage** is a collection of transformations with **narrow dependencies**, meaning that one partition of the output depends on only one partition of each input.\n",
    "- The boundaries between stages correspond to **wide dependencies**, meaning that one partition of the output depends on multiple partitions of some input, requiring a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Graph Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregel\n",
    "\n",
    "- In Pregel, graph processing problems are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology.\n",
    "- This **vertex-centric approach** is flexible enough to express a broad set of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "\n",
    "- **Unit of Work**: A *partition* consisting of a set of vertices and their outgoing edges.\n",
    "- **Coordination**: A single *master* with many *workers*.\n",
    "\n",
    "\n",
    "1. The master determines the number of partitions and distributes the partitions to each worker.\n",
    "2. The master assigns a portion of the user's input to each worker.\n",
    "    1. If the worker is assigned a vertex that belongs to its partition of the graph, then the worker updates the the state of the vertex.\n",
    "        - *Vertex's Current Value*\n",
    "        - *Vertex's Outgoing Edges*\n",
    "        - *Vertex's Activity Flag*\n",
    "    2. If the worker is assigned a vertex that does not belong to its partition of the graph, then the worker sends a message containing the vertex and its edges to the appropriate remote peer.\n",
    "    3. All the input vertices are marked as active.\n",
    "3. The master instructs each worker to perform a **superstep**.\n",
    "    1. The worker loops to compute through its active vertices.\n",
    "        1. Asynchronously execute a user-defined function on each vertex.\n",
    "        2. Receive messages sent in the previous superstep.\n",
    "        3. Send messages to be received in the next superstep.\n",
    "        4. Vertices can modify their value.\n",
    "        5. Vertices can modify the values of their edges.\n",
    "        6. Vertices can add or remove edges.\n",
    "        7. Vertices can deactivate themselves.  \n",
    "    2. The worker notifies the master how many vertices will be active in the next superstep.\n",
    "4. Repeat *Step 3* until all the vertices are inactive.\n",
    "\n",
    "\n",
    "- **Important Note**: An active vertex is reactivated when it receives a message.\n",
    "- **Bulk Synchronous Parallel**: Workers compute asynchronously within each superstep, and communicate only between supersteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiners and Aggregators\n",
    "\n",
    "- **Combiners**: An optional component which reduces the amount of data exchanged over the network and the number of messages.\n",
    "    - i.e., A *commutative* and *associative* user-defined function.\n",
    "- **Aggregators**: An optional component which computes aggregate statistics from vertex-reported values.\n",
    "    1. Workers aggregate values from their vertices during each supersep.\n",
    "    2. At the end of each superstep, the values from the workers are aggregated in a tree structure, and the value from the root of the tree is sent to the master.\n",
    "    3. The master shares the value with all vertices in the next superstep.\n",
    "- An aggregator is useful for detecting convergence conditions for vertices to transition to the inactive state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance\n",
    "\n",
    "- **Key Idea**: *Checkpointing*\n",
    "- At the beginning of a superstep, the master instructs the workers to save the state of their partitions to persistent storage.\n",
    "    - Vertex Values.\n",
    "    - Edge Values.\n",
    "    - Incoming Messages.\n",
    "- The master separately saves the aggregator values.\n",
    "- Worker failures are detected using regular pings from the master to the workers.\n",
    "- When one or more workers fail, the master reassigns graph partitions to the currently available set of workers, and they all reload their partition state from the most recent available checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency and Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations for Replication\n",
    "\n",
    "- *Increased Reliability*\n",
    "- *Increased Throughput*\n",
    "- *Decreased Latency*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicated Data Store\n",
    "\n",
    "- In a **replicated data store**, each data object is replicated at multiple hosts.\n",
    "    - **Local Replica**: *Same Hosts*\n",
    "    - **Remote Replica**: *Different Hosts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Models\n",
    "\n",
    "1. *Sequential Consistency*\n",
    "2. *Causal Consistency*\n",
    "3. *Linearizability*\n",
    "4. *Eventual Consistency*\n",
    "5. *Session Guarantees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Consistency\n",
    "\n",
    "- A data store is sequentially consistent when the result of any execution is the same as if the operations by all processes on the data were executed in some sequential order and the operations of each individual process appear in this sequence in the order specified by its program.\n",
    "\n",
    "![Positive Sequential Consistency Example](images/Consistency_1.png)\n",
    "*Positive Sequential Consistency Example*\n",
    "\n",
    "![Negative Sequential Consistency Example](images/Consistency_2.png)\n",
    "*Negative Sequential Consistency Example*\n",
    "\n",
    "1. `R(x)a` and `R(x)b` conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Consistency\n",
    "\n",
    "- A data store is causally consistent when writes related by the \"causally precedes\" relation must be seen by all processes in the same oder.\n",
    "- Concurrent writes may be seen in a different order on different machines.\n",
    "- **\"Causally precedes\"** is the transitive closure of two rules.\n",
    "    1. Operation A causally precedes operation B if A occurs before B in the same process.\n",
    "    2. Operation A causally precedes operation B if B reads a value written by A.\n",
    "- If operations A and B are concurrent (no \"causally precedes\"), then A and B can be read in either order.\n",
    "\n",
    "![Positive Causal Consistency Example](images/Consistency_3.png)\n",
    "*Positive Causal Consistency Example*\n",
    "\n",
    "1. `W(x)a` causally precedes `R(x)a`.\n",
    "2. `R(x)a` causally precedes `W(x)b`.\n",
    "3. `W(x)b` and `W(x)c` are concurrent.\n",
    "4. Therefore, the reads must occur in the following sequences:\n",
    "    1. A, B, C, or\n",
    "    2. A, C, B.\n",
    "    \n",
    "![Negative Causal Consistency Example](images/Consistency_4.png)\n",
    "*Negative Causal Consistency Example*\n",
    "\n",
    "1. `W(x)a` causally precedes `R(x)a`.\n",
    "2. `R(x)a` causally precedes `W(x)b`.\n",
    "3. However, P3's `R(x)b` before `R(x)a` violates causal consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearizability\n",
    "\n",
    "- A data store is linearizable when the result of any execution is the same as if the operations by all processes on the data store were executed in some sequential order that extends the \"happens before\" relation.\n",
    "- If operation A finishes before operation B begins, then A must precede B in the sequential order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eventual Consistency\n",
    "\n",
    "- If no updates take palce for a long time, all replicas will gradually become consistent.\n",
    "- Allows different processes to observe write operations taking effect in different orders, even when these write operations are related by \"causally precedes\" or \"happens before\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Guarantees\n",
    "\n",
    "- **Session Guarantees**: Restrict the behavior of operations applied by a single process in a single session.\n",
    "- **Monotonic Reads**: If a process reads $x$, any successive reads on $x$ by that process will always return the same value or a more recent value.\n",
    "- **Monotonic Writes**: A write by a process on $x$ is completed before any successive write on $x$ by the same process.\n",
    "- **Read Your Own Writes**: The effect of a write operation by a process on $x$ will always be seen by a successive read on $x$ by the same process.\n",
    "- **Writes Follow Reads**: A write by a process on $x$ following a previous read on $x$ by the same process is guaranteed to take place on the same or a more recent value of $x$ that was read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary-Based Replication Protocols\n",
    "\n",
    "- In a primary-based protocol, each $x$ in the data store has an associated primary, which is responsible for coordinating writes on $x$.\n",
    "- If the primary replica fails, then one of the backup replicas may take over as the new primary.\n",
    "    - *Disadvantage*: If the network is partitioned, the cluster can become **split-brain** such that one replica in each partition believes its the primary replica; hence, a divergence of state.\n",
    "    \n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "- *Strong Consistency*\n",
    "\n",
    "##### Disadvantages\n",
    "\n",
    "- *Performance Bottlenecks*\n",
    "- *Loss of Availability*\n",
    "    \n",
    "#### Remote-Write Protocol\n",
    "\n",
    "![Remote-Write Protocol Example](images/Replication_1.png)\n",
    "    \n",
    "- **Remote-Write**: The primary replica is generally stationary and therefore must be updated remotely by other servers.\n",
    "\n",
    "#### Local-Write Protocol\n",
    "\n",
    "![Local-Write Protocol Example](images/Replication_2.png)\n",
    "\n",
    "- **Local-Write**: The primary replica migrates from server to server, allowing local updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quorum-Based Replication Protocols\n",
    "\n",
    "- In a quorum-based protocol, all replicas are allowed to receive updates and reads, but operations are required to be accepted by a sufficiently large subset of replicas called a **write quorum** or a **read quorum**.\n",
    "\n",
    "\n",
    "#### Requirements of Write and Read Quorums\n",
    "\n",
    "![Write and Read Quorums Examples](images/Replication_3.png)\n",
    "\n",
    "- Let $N$ be the total number of replicas.\n",
    "- Let $N_{W}$ be the size of the write quorum.\n",
    "- Let $N_{R}$ be the size of the read quorum.\n",
    "- The following two rules must be satisfied:\n",
    "    1. $N_{R} + N_{E} > N$; Read and write quorums must overlap.\n",
    "    2. $N_{W} + N_{W} > N$; Two write quorums must overlap.\n",
    "- The first rule enables detection of **read-write conflicts**.\n",
    "     - Read-write conflicts occur when one process wants to update data item while another is concurrently attempting to read that item.\n",
    "- The second rule enables detection of **write-write conflicts**.\n",
    "    - Write-write conflicts occur when two processes want to perform an update on the same data.\n",
    "\n",
    "#### Partial Quorums\n",
    "\n",
    "- Derivatives of Anamzon's Dynamo, allow various degress of consistency by tuning $N_{R}$ and $N_{W}$.\n",
    "    - **Strong Consistency**: $N_{R} + N_{W} > N$\n",
    "    - **Weak Consistency**: $N_{R} + N_{W} \\le N$\n",
    "- **Important Note 1**: The strong consistency mode does not avoid write-write conflicts.\n",
    "- **Important Note 2**: The weak consistency mode does not avoid read-write conflicts or write-write conflicts.\n",
    "- To resolve write-write conflicts, updates are tagged with timestamps, and resolution policies such as **last-write wins** or **vector clocks** are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eventually-Consistent Replication\n",
    "\n",
    "- A server that receives an update replies with an acknowledgement to the client first, and then propagates the update lazily to the remaining replicas.\n",
    "- If a replica is unreachable, then it can be updated later using an **anti-entropy** mechanism.\n",
    "    - e.g., Replicas may periodically exchange hashes of data to detect discrepancies using Merkle trees.\n",
    "    - e.g., Updates can be timestamped to enable determination of the latest version of a data item."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "11.0.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
