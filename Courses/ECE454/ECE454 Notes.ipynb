{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 454 - Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed System\n",
    "\n",
    "- **Distributed System**: A collection of autonomous computing elements that appears to its users as a single coherent system.\n",
    "\n",
    "#### Motivations for Distributed Systems\n",
    "\n",
    "1. Resource Sharing\n",
    "2. Simplify Processes by Integrating Multiple Systems\n",
    "3. Limitations in Centralized Systems: Weak/Unreliable\n",
    "4. Distributed/Mobile Users\n",
    "\n",
    "#### Goals for Distributed Systems\n",
    "\n",
    "1. Resource Sharing\n",
    "    - CPUs, Data, Peripherals, Storage.\n",
    "2. Transparency\n",
    "    - Access, Location, Migration, Relocation, Replication, Concurrency, Failure.\n",
    "3. Open\n",
    "    - Interoperability, Composability, Extensibility.\n",
    "4. Scalable\n",
    "    - Size, Geography, Administration.\n",
    "    \n",
    "#### Types of Distributed Systems\n",
    "\n",
    "- Web Services\n",
    "- High Performance Computing, Cluster Computing, Cloud Computing, Grid Computing\n",
    "- Transaction Processing\n",
    "- Enterprise Application Integration\n",
    "- Internet of Things, Sensor Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware\n",
    "\n",
    "- **Middleware**: A layer of software that separates applications from the underlying platforms.\n",
    "    - Supports Heterogeneous Computers/Networks.\n",
    "    - *e.g.*: Communication, Transactions, Service Composition, Reliability.\n",
    "    - **Single-System View**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Techniques\n",
    "\n",
    "1. *Hiding Communication Latencies*: At Server vs. At Client?\n",
    "2. *Partitioning*\n",
    "3. *Replication*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallacies of Networked and Distributed Computing\n",
    "\n",
    "1. Network is reliable.\n",
    "2. Network is secure.\n",
    "3. Network is homogeneous.\n",
    "4. Topology is static.\n",
    "5. Latency is zero.\n",
    "6. Bandwidth is infinite.\n",
    "7. Transport cost is zero.\n",
    "8. There is only one administrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory vs. Message Passing\n",
    "\n",
    "- **Shared Memory**:\n",
    "    - Less Scalable\n",
    "    - Faster\n",
    "    - CPU-Intensive Problems\n",
    "    - Parallel Computing\n",
    "- **Message Passing**:\n",
    "    - More Scalable\n",
    "    - Slower\n",
    "    - Resource Sharing / Coordination Problems\n",
    "    - Distributed Computing\n",
    "- Apache Hadoop is an example of a hybrid computing framework that uses message passing at a broad-view and shared memory at a detailed-view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud and Grid Computing\n",
    "\n",
    "- **IaaS**: Infrastructure as a Service\n",
    "    - VM Computation, Block File Storage\n",
    "- **PaaS**: Platform as a Service\n",
    "    - Software Frameworks, Databases\n",
    "- **SaaS**: Software as a Service\n",
    "    - Web Services, Business Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Processing Systems\n",
    "\n",
    "- **Transaction Processing Monitor**: Coordinates Distributed Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "- **Component**: A modular unit with well-defined interfaces.\n",
    "- **Connector**: A mechanism that mediates communication, coordination, or cooperation among components.\n",
    "- **Software Architecture**: Organization of software components.\n",
    "- **System Architecture**: Instantiation of software architecture in which software components are placed on real machines.\n",
    "- **Autonomic System**: Adapts to its environment by monitoring its own behavior and reacting accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectural Styles\n",
    "\n",
    "- Layered\n",
    "    - *Note: Assignment Topic*\n",
    "- Object-Based\n",
    "- Data-Centered\n",
    "- Event-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Architecture\n",
    "\n",
    "![Layers](images/Architecture_1.png)\n",
    "\n",
    "- **Examples**:\n",
    "    - Database Server, Application Server, Client\n",
    "    - SSH Server, SSH Client\n",
    "- Requests Flow Down Stack\n",
    "- Responses Flow Up Stack\n",
    "- *Handle-Upcall*: Async Notification\n",
    "    - Subscribe with Handle\n",
    "    - Publish with Upcall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client-Server Interactions\n",
    "\n",
    "![Client-Server Interactions](images/Architecture_2.png)\n",
    "\n",
    "- *Bolded Lines* = Busy\n",
    "- *Dashed Lines* = Idle\n",
    "- **Client**: Initiates with a Request\n",
    "- **Server**: Follows with a Response\n",
    "- **Total Round-Trip Time**: $(N - 1) \\times t_{\\text{Request-Response}}$\n",
    "    - Layering can reduce the amount of processing time per layer, but the additional communication overhead between the layers introduces diminishing returns.\n",
    "- An intermediate layer can be both a client and a server to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Tiered Architecture\n",
    "\n",
    "- Logical Software Layers $\\mapsto$ Physical Tiers\n",
    "    - *Trade-Offs*: Ease of Maintenance vs. Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal vs. Vertical Distribution\n",
    "\n",
    "- **Vertical Distribution**: When the logical layers of a system are organized as separate physical tiers.\n",
    "    - *Performance*: High.\n",
    "    - *Scalability*: Low.\n",
    "    - *Dependability*: Low-Medium.\n",
    "- **Horizontal Distribution**: When one logical layer is split across multiple machines - **sharding**.\n",
    "    - *Performance*: Low.\n",
    "    - *Scalability*: High.\n",
    "    - *Dependability*: Medium-High."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object-Based Architecture\n",
    "\n",
    "- In an object-based architecture, components communicate using remote object references and method calls.\n",
    "\n",
    "#### Problems with Object-Based Architecture\n",
    "\n",
    "- Complex Communication Interfaces\n",
    "- Complex Communication Costs\n",
    "- Not Scalable\n",
    "- Not Language Agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Centered Architecture\n",
    "\n",
    "- In a data-centered architecture, components communicate by accessing a shared data repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-Based Architecture\n",
    "\n",
    "![Publish/Subscribe Middleware](images/Architecture_3.png)\n",
    "\n",
    "- In an event-based architecture, components communicate by propagating events using a publish/subscribe system.\n",
    "\n",
    "#### Handling Asynchronous Delivery Failure\n",
    "\n",
    "- *At-Least Once Delivery*: Do Retransmit\n",
    "- *At-Most Once Delivery*: Do Not Retransmit\n",
    "- *Exactly Once Delivery*: Unknown/Unachievable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer-to-Peer Systems\n",
    "\n",
    "![Chord's Finger Table](images/Architecture_4.png)\n",
    "\n",
    "- In a peer-to-peer system, decentralized processes are organized in an overlay network that defines a set of communication channels.\n",
    "- In a peer-to-peer, distributed hash table, a keyspace is represented by a consistent hash ring on top of which nodes partition ranges amongst themselves.\n",
    "- The mappings of partition ranges to nodes are maintained by a finger table which can be queried in a logarithm process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Architectures\n",
    "\n",
    "- BitTorrent is an example of a hybrid architecture combining a client-server architecture and a peer-to-peer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Management\n",
    "\n",
    "![Self-Management Systems](images/Architecture_5.png)\n",
    "\n",
    "- In self-management, systems use a feedback control loop that monitors system behaviors and adjusts system operations.\n",
    "- **Assignment Note**: Useful for Unknown Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPC\n",
    "\n",
    "- **Inter-Process Communication (IPC)**: Expensive b/c Context Switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n",
    "\n",
    "- Typically, an operating system kernel support multi-threading through **lightweight processes (LWP)**.\n",
    "- **Assignment Note**: Do Not Spawn Too Many Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Threaded Servers\n",
    "\n",
    "- **Dispatcher/Worker Design**: A dispatcher thread receives requests from the network and feeds them to a pool of worker threads.\n",
    "- **Assignment Note**: Useful for Assignment 1 & Partition into Sequential Work and Parallel Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware and Software Interfaces\n",
    "\n",
    "![Hardware and Software Interfaces](images/Processes_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtualization\n",
    "\n",
    "![VMs](images/Processes_2.png)\n",
    "\n",
    "- **Advantage**:\n",
    "    - Portability\n",
    "    - Live Migration of VMs\n",
    "    - Replication for Availability/Fault Tolerance\n",
    "- **Disadvantage**:\n",
    "    - Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Clusters\n",
    "\n",
    "![Three Physical Tier](images/Processes_3.png)\n",
    "\n",
    "- **Assignment Note**: Useful for Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Network Model\n",
    "\n",
    "![Layered Network Model](images/Communication_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Procedure Calls\n",
    "\n",
    "- **Remote Procedure Calls**: A transient communication abstraction implemented using a client-server protocol.\n",
    "- **Client Stub**: Translate a RPC on the client.\n",
    "- **Server Stub**: Translate a RPC on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of a RPC\n",
    "\n",
    "![Steps of a RPC](images/Communication_2.png)\n",
    "\n",
    "\n",
    "1. The client process invokes the client stub using an ordinary procedure call.\n",
    "2. The client stub builds a message and passes it to the client's OS.\n",
    "3. The client's OS sends the message to the server's OS.\n",
    "4. The server's OS delivers the message to the server stub.\n",
    "5. The server stub unpacks the parameters and invokes the appropriate service handler in the server process.\n",
    "6. The **service handler** does the work and returns the result to the server stub.\n",
    "7. The server stub packs the result into a message and passes it to the server's OS.\n",
    "8. The server's OS sends the message to the client's OS.\n",
    "9. The client's OS delivers the message to the client stub.\n",
    "10. The client stub unpacks the result and returns it to the client process.\n",
    "\n",
    "\n",
    "- **Parameter Marshalling**: Packing Parameter $\\to$ Message\n",
    "    - Processor Architectures, Network Protocols, and VMs $\\implies$ **Little-Endian** vs. **Big-Endian**\n",
    "- **Number of System Calls**: 4\n",
    "    1. Client Process $\\to$ Client OS Socket\n",
    "    2. Server OS Socket $\\to$ Server Process\n",
    "    3. Server Process $\\to$ Server OS Socket\n",
    "    4. Client OS Socket $\\to$ Client Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining RPC Interfaces\n",
    "\n",
    "- **Interface Definition Language (IDL)**: Specify RPC Signatures $\\to$ Client/Server Stubs\n",
    "    - High-Level Format\n",
    "    - Parameter Ordering\n",
    "    - Byte Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous vs. Asynchronous RPCs\n",
    "\n",
    "- **Synchronous RPC**: The client blocks to wait for the return value.\n",
    "- **Asynchronous RPC**: The client blocks to wait for the server acknowledgement of the receipt of the request.\n",
    "- **One-Way RPC**: The client does not block to wait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Queuing Model\n",
    "\n",
    "![Message Queue Interface](images/Communication_3.png)\n",
    "\n",
    "- **Message Queue**: Alternative to RPCs\n",
    "- **Persistent Communication**: Loose Coupling between Client/Server\n",
    "    - *Advantage*: Resilient to Client/Server Hardware Failure\n",
    "    - *Disadvantage*: Guaranteed Delivery = Impossible\n",
    "- **Message-Oriented Middleware (MOM)**: Asynchronous Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Coupling\n",
    "\n",
    "- **Referential Coupling**: When one process explicitly references another.\n",
    "    - *Positive Example*: RPC client connects to server using an IP address and a port number\n",
    "    - *Negative Example*: Publisher inserts a news item into a pub-sub system without knowing which subscriber will read it.\n",
    "- **Temporal Coupling**: Communicating processes must both be up and running.\n",
    "    - *Positive Example*: A client cannot execute a RPC if the server is down.\n",
    "    - *Negative Example*: A producer appends a job to a message queue today, and a consumer extracts the job tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPC vs. MOM\n",
    "\n",
    "#### RPC\n",
    "\n",
    "- Used mostly for two-way communication, particularly where the client requires immediate response from the server.\n",
    "- The middleware is linked into the client and the server processes.\n",
    "- Tighter coupling means that server failure can prevent client from making progress.\n",
    "\n",
    "#### MOM\n",
    "\n",
    "- Used mostly for one-way communication where one party does not require an immediate response from another.\n",
    "- The middleware is a separate component between the sender/publisher/producer and the receiver/subscriber/consumer.\n",
    "- Looser coupling isolates one process from another which contributes to flexibility and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed File Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Remote Files\n",
    "\n",
    "![DFS Models](images/DFS_1.png)\n",
    "\n",
    "- **Remote Access Model**\n",
    "- **Upload/Download Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network File System (NFS)\n",
    "\n",
    "![Overview of NFS](images/DFS_2.png)\n",
    "\n",
    "- *Supports Client-Side Caching*\n",
    "    - Modifications are flushed to the server when the client closes the file.\n",
    "    - Consistency is implementation dependent.\n",
    "\n",
    "![Authority Delegation](images/DFS_3.png)\n",
    "    \n",
    "- *Supports Authority Delegation*\n",
    "    - A server can delegate authority to a client and recall it through a callback mechanism.\n",
    "    \n",
    "![Compound Procedure](images/DFS_4.png)\n",
    "    \n",
    "- *Supports Compound Procedures*\n",
    "    - Multiple Round Trips to Single Round Trip\n",
    "\n",
    "![Partial Exports](images/DFS_5.png)\n",
    "\n",
    "- *Supports Partial Exports*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google File System (GFS)\n",
    "\n",
    "![Google File System](images/DFS_6.png)\n",
    "\n",
    "- **GFS**: A distributed file system that stripes files across inexpensive commodity servers without RAID.\n",
    "    - *Layered Above Linux File System*\n",
    "    - *Fault Tolerance Through Software*\n",
    "- **GFS Master**: *Stores Metadata About Files/Chunks*\n",
    "    - *Metadata Cache in Main Memory*\n",
    "    - *Updated Log in Local Storage*\n",
    "    - *Periodically Polls Client Servers for Consistency*\n",
    "    \n",
    "#### Reading a File\n",
    "\n",
    "1. A client sends the file name and chunk index to the master.\n",
    "2. The master responds with a contact address.\n",
    "3. The client then pulls data directly from a chunk server, bypassing the master.\n",
    "\n",
    "#### Updating a File\n",
    "\n",
    "1. The client pushes its updates to the nearest chunk server holding the data.\n",
    "2. The nearest chunk server pushes the update to the next closest chunk server holding the data, and so on.\n",
    "3. When all replicas have received the data, the primary chunk server assigns a sequence number to the update operation and passes it on to the secondary chunk servers.\n",
    "4. The primary replica informs the client that the update is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Sharing Semantics\n",
    "\n",
    "![File Sharing Semantics](images/DFS_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Architecture\n",
    "\n",
    "![Hadoop High-Level Architecture](images/Hadoop_1.png)\n",
    "\n",
    "- Transform lists of input data elements into lists of output data elements by applying *Mappers* and *Reducers*\n",
    "    - *Immutable Data*\n",
    "    - *No Communication*\n",
    "    \n",
    "#### Mapper\n",
    "\n",
    "- A list of input data elements are iterated and individually transformed into zero or more output data elements.\n",
    "\n",
    "#### Reducer\n",
    "\n",
    "- A list of input data elements are iterated and individually aggregated into a single output data element.\n",
    "\n",
    "#### Combiner\n",
    "\n",
    "- An optional component that consumes the outputs of a mapper to produce a summary as the inputs for a reducer.\n",
    "\n",
    "#### Terms\n",
    "\n",
    "- **InputSplit**: A unit of work assigned to one map task.\n",
    "    - Usually corresponds to a chunk of an input file.\n",
    "    - Each record in a file belongs to exactly one input split and the framework takes care of dealing with record boundaries.\n",
    "- **InputFormat**: Determines how the input files are parsed, and defines the input splits.\n",
    "- **OutputFormat**: Determines how the output files are formatted.\n",
    "- **RecordReader**: Reads data from an input split and creates key-value pairs for the mapper.\n",
    "- **RecordWriter**: Writes key-value pairs to output files.\n",
    "- **Partitioner**: Determines which partition a given key-value pair will go to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow\n",
    "\n",
    "![MapReduce Data Flow 1](images/Hadoop_2.png)\n",
    "\n",
    "![MapReduce Data Flow 2](images/Hadoop_3.png)\n",
    "\n",
    "- **Shuffle**: The process of partitioning by reducer, sorting and copying data partitions from mappers to reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance\n",
    "\n",
    "- *Primarily: Restart Failed Tasks*\n",
    "    1. Individual *TaskTrackers* periodically emit a heartbeat to the *JobTracker*.\n",
    "    2. If a *TaskTracker* fails to emit a heartbeat to the *JobTracker*, the *JobTracker* assumes that the *TaskTracker* crashed.\n",
    "    3. If the failed node was mapping, then other *TaskTrackers* will be asked to re-execute all the map tasks previously run by the failed *TaskTracker*.\n",
    "    - *Must be Side-Effect Free*\n",
    "    4. If the failed node was reducing, then other *TaskTrackers* will be asked to re-execute all reduce tasks that were in progress on the failed *TaskTracker*.\n",
    "    - *Must be Side-Effect Free*\n",
    "- *Secondarily: Speculative Execution*\n",
    "    - If some **straggler** nodes rate limit the rest of the program, Hadoop will schedule redundant copies of remaining tasks across several nodes which do not have other work to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Design Patterns\n",
    "\n",
    "#### Counts and Summations\n",
    "\n",
    "- A mapper can emit a tuple of an element and one for each element.\n",
    "- A mapper can aggregate the counts for each element and emit a tuple of the element and its count.\n",
    "- A combiner can aggregate the counts across all the elements processed by a mapper.\n",
    "\n",
    "#### Selection\n",
    "\n",
    "- A mapper can emit a tuple for each element that satisfies a predicate.\n",
    "\n",
    "#### Projection\n",
    "\n",
    "- A mapper can emit a tuple whose fields are a subset of each element.\n",
    "- A reducer can eliminate duplicates.\n",
    "\n",
    "#### Inverted Index\n",
    "\n",
    "- A mapper can emit a tuple of a value and a key in that specific order.\n",
    "- A reducer can aggregate all the keys for a distinct value.\n",
    "\n",
    "#### Cross-Correlation\n",
    "\n",
    "- **Problem**: Given a set of tuples of items, for each possible pair of items, calculate the number of tuples where these items co-occur.\n",
    "\n",
    "##### Pairs Approach (Slow)\n",
    "\n",
    "```\n",
    "class Mapper\n",
    "  method Map(void, items [i1, i2, ...])\n",
    "    for all item i in [i1, i2, ...]\n",
    "      for all item j in [i1, i2, ...] such that j > i\n",
    "        Emit(pair [i, j], count 1)\n",
    "\n",
    "class Reducer\n",
    "  method Reduce(pair [i, j], counts [c1, c2, ...])\n",
    "    s = sum([c1, c2, ...])\n",
    "    Emit(pair [i, j], count s)\n",
    "```\n",
    "\n",
    "##### Stripes Approach (Fast)\n",
    "\n",
    "```\n",
    "class Mapper\n",
    "  method Map(void, items [i1, i2, ...])\n",
    "    for all item i in [i1, i2, ...]\n",
    "      H = new AssociativeArray : item -> counter\n",
    "      for all item j in [i1, i2, ...] such that j > i\n",
    "        H{j} = H{j} + 1\n",
    "      Emit(item i, stripe H)\n",
    "      \n",
    "class Reducer\n",
    "  method Reduce(item i, stripes [H1, H2, ...])\n",
    "    H = new AssociativeArray : item -> counter\n",
    "    H = merge-sum([H1, H2, ...])\n",
    "    for all item j in H.keys()\n",
    "      Emit(pair [i, j], H{j})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "- **RDDs** are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage\n",
    "\n",
    "![Lineage](images/Spark_1.png)\n",
    "\n",
    "- A **lineage** is a directed acyclic graph that expresses the dependencies between RDDs such that a RDD can be rebuilt in the event of a failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and Actions\n",
    "\n",
    "- **Transformations**: Operations that convert one RDDs or a pair of RDDs into another RDD.\n",
    "- **Actions**: Operations that convert a RDD into an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow vs. Wide Dependencies\n",
    "\n",
    "![Dependencies](images/Spark_2.png)\n",
    "\n",
    "![Execution](images/Spark_3.png)\n",
    "\n",
    "- When an action is invoked on a RDD, the Spark scheduler examines the lineage graph of the RDD and builds a directed acyclic graph of transformations.\n",
    "- The transformations in the DAG are grouped into **stages**.\n",
    "- A **stage** is a collection of transformations with **narrow dependencies**, meaning that one partition of the output depends on only one partition of each input.\n",
    "- The boundaries between stages correspond to **wide dependencies**, meaning that one partition of the output depends on multiple partitions of some input, requiring a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Graph Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregel\n",
    "\n",
    "- In Pregel, graph processing problems are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology.\n",
    "- This **vertex-centric approach** is flexible enough to express a broad set of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "\n",
    "- **Unit of Work**: A *partition* consisting of a set of vertices and their outgoing edges.\n",
    "- **Coordination**: A single *master* with many *workers*.\n",
    "\n",
    "\n",
    "1. The master determines the number of partitions and distributes the partitions to each worker.\n",
    "2. The master assigns a portion of the user's input to each worker.\n",
    "    1. If the worker is assigned a vertex that belongs to its partition of the graph, then the worker updates the the state of the vertex.\n",
    "        - *Vertex's Current Value*\n",
    "        - *Vertex's Outgoing Edges*\n",
    "        - *Vertex's Activity Flag*\n",
    "    2. If the worker is assigned a vertex that does not belong to its partition of the graph, then the worker sends a message containing the vertex and its edges to the appropriate remote peer.\n",
    "    3. All the input vertices are marked as active.\n",
    "3. The master instructs each worker to perform a **superstep**.\n",
    "    1. The worker loops to compute through its active vertices.\n",
    "        1. Asynchronously execute a user-defined function on each vertex.\n",
    "        2. Receive messages sent in the previous superstep.\n",
    "        3. Send messages to be received in the next superstep.\n",
    "        4. Vertices can modify their value.\n",
    "        5. Vertices can modify the values of their edges.\n",
    "        6. Vertices can add or remove edges.\n",
    "        7. Vertices can deactivate themselves.  \n",
    "    2. The worker notifies the master how many vertices will be active in the next superstep.\n",
    "4. Repeat *Step 3* until all the vertices are inactive.\n",
    "\n",
    "\n",
    "- **Important Note**: An active vertex is reactivated when it receives a message.\n",
    "- **Bulk Synchronous Parallel**: Workers compute asynchronously within each superstep, and communicate only between supersteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiners and Aggregators\n",
    "\n",
    "- **Combiners**: An optional component which reduces the amount of data exchanged over the network and the number of messages.\n",
    "    - i.e., A *commutative* and *associative* user-defined function.\n",
    "- **Aggregators**: An optional component which computes aggregate statistics from vertex-reported values.\n",
    "    1. Workers aggregate values from their vertices during each supersep.\n",
    "    2. At the end of each superstep, the values from the workers are aggregated in a tree structure, and the value from the root of the tree is sent to the master.\n",
    "    3. The master shares the value with all vertices in the next superstep.\n",
    "- An aggregator is useful for detecting convergence conditions for vertices to transition to the inactive state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance\n",
    "\n",
    "- **Key Idea**: *Checkpointing*\n",
    "- At the beginning of a superstep, the master instructs the workers to save the state of their partitions to persistent storage.\n",
    "    - Vertex Values.\n",
    "    - Edge Values.\n",
    "    - Incoming Messages.\n",
    "- The master separately saves the aggregator values.\n",
    "- Worker failures are detected using regular pings from the master to the workers.\n",
    "- When one or more workers fail, the master reassigns graph partitions to the currently available set of workers, and they all reload their partition state from the most recent available checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency and Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations for Replication\n",
    "\n",
    "- *Increased Reliability*\n",
    "- *Increased Throughput*\n",
    "- *Decreased Latency*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicated Data Store\n",
    "\n",
    "- In a **replicated data store**, each data object is replicated at multiple hosts.\n",
    "    - **Local Replica**: *Same Hosts*\n",
    "    - **Remote Replica**: *Different Hosts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Models\n",
    "\n",
    "1. *Sequential Consistency*\n",
    "2. *Causal Consistency*\n",
    "3. *Linearizability*\n",
    "4. *Eventual Consistency*\n",
    "5. *Session Guarantees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Consistency\n",
    "\n",
    "- A data store is sequentially consistent when the result of any execution is the same as if the operations by all processes on the data were executed in some sequential order and the operations of each individual process appear in this sequence in the order specified by its program.\n",
    "\n",
    "![Positive Sequential Consistency Example](images/Consistency_1.png)\n",
    "*Positive Sequential Consistency Example*\n",
    "\n",
    "![Negative Sequential Consistency Example](images/Consistency_2.png)\n",
    "*Negative Sequential Consistency Example*\n",
    "\n",
    "1. `R(x)a` and `R(x)b` conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Consistency\n",
    "\n",
    "- A data store is causally consistent when writes related by the \"causally precedes\" relation must be seen by all processes in the same oder.\n",
    "- Concurrent writes may be seen in a different order on different machines.\n",
    "- **\"Causally precedes\"** is the transitive closure of two rules.\n",
    "    1. Operation A causally precedes operation B if A occurs before B in the same process.\n",
    "    2. Operation A causally precedes operation B if B reads a value written by A.\n",
    "- If operations A and B are concurrent (no \"causally precedes\"), then A and B can be read in either order.\n",
    "\n",
    "![Positive Causal Consistency Example](images/Consistency_3.png)\n",
    "*Positive Causal Consistency Example*\n",
    "\n",
    "1. `W(x)a` causally precedes `R(x)a`.\n",
    "2. `R(x)a` causally precedes `W(x)b`.\n",
    "3. `W(x)b` and `W(x)c` are concurrent.\n",
    "4. Therefore, the reads must occur in the following sequences:\n",
    "    1. A, B, C, or\n",
    "    2. A, C, B.\n",
    "    \n",
    "![Negative Causal Consistency Example](images/Consistency_4.png)\n",
    "*Negative Causal Consistency Example*\n",
    "\n",
    "1. `W(x)a` causally precedes `R(x)a`.\n",
    "2. `R(x)a` causally precedes `W(x)b`.\n",
    "3. However, P3's `R(x)b` before `R(x)a` violates causal consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearizability\n",
    "\n",
    "- A data store is linearizable when the result of any execution is the same as if the operations by all processes on the data store were executed in some sequential order that extends the \"happens before\" relation.\n",
    "- If operation A finishes before operation B begins, then A must precede B in the sequential order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eventual Consistency\n",
    "\n",
    "- If no updates take palce for a long time, all replicas will gradually become consistent.\n",
    "- Allows different processes to observe write operations taking effect in different orders, even when these write operations are related by \"causally precedes\" or \"happens before\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Guarantees\n",
    "\n",
    "- **Session Guarantees**: Restrict the behavior of operations applied by a single process in a single session.\n",
    "- **Monotonic Reads**: If a process reads $x$, any successive reads on $x$ by that process will always return the same value or a more recent value.\n",
    "- **Monotonic Writes**: A write by a process on $x$ is completed before any successive write on $x$ by the same process.\n",
    "- **Read Your Own Writes**: The effect of a write operation by a process on $x$ will always be seen by a successive read on $x$ by the same process.\n",
    "- **Writes Follow Reads**: A write by a process on $x$ following a previous read on $x$ by the same process is guaranteed to take place on the same or a more recent value of $x$ that was read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary-Based Replication Protocols\n",
    "\n",
    "- In a primary-based protocol, each $x$ in the data store has an associated primary, which is responsible for coordinating writes on $x$.\n",
    "- If the primary replica fails, then one of the backup replicas may take over as the new primary.\n",
    "    - *Disadvantage*: If the network is partitioned, the cluster can become **split-brain** such that one replica in each partition believes its the primary replica; hence, a divergence of state.\n",
    "    \n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "- *Strong Consistency*\n",
    "\n",
    "##### Disadvantages\n",
    "\n",
    "- *Performance Bottlenecks*\n",
    "- *Loss of Availability*\n",
    "    \n",
    "#### Remote-Write Protocol\n",
    "\n",
    "![Remote-Write Protocol Example](images/Replication_1.png)\n",
    "    \n",
    "- **Remote-Write**: The primary replica is generally stationary and therefore must be updated remotely by other servers.\n",
    "\n",
    "#### Local-Write Protocol\n",
    "\n",
    "![Local-Write Protocol Example](images/Replication_2.png)\n",
    "\n",
    "- **Local-Write**: The primary replica migrates from server to server, allowing local updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quorum-Based Replication Protocols\n",
    "\n",
    "- In a quorum-based protocol, all replicas are allowed to receive updates and reads, but operations are required to be accepted by a sufficiently large subset of replicas called a **write quorum** or a **read quorum**.\n",
    "\n",
    "\n",
    "#### Requirements of Write and Read Quorums\n",
    "\n",
    "![Write and Read Quorums Examples](images/Replication_3.png)\n",
    "\n",
    "- Let $N$ be the total number of replicas.\n",
    "- Let $N_{W}$ be the size of the write quorum.\n",
    "- Let $N_{R}$ be the size of the read quorum.\n",
    "- The following two rules must be satisfied:\n",
    "    1. $N_{R} + N_{E} > N$; Read and write quorums must overlap.\n",
    "    2. $N_{W} + N_{W} > N$; Two write quorums must overlap.\n",
    "- The first rule enables detection of **read-write conflicts**.\n",
    "     - Read-write conflicts occur when one process wants to update data item while another is concurrently attempting to read that item.\n",
    "- The second rule enables detection of **write-write conflicts**.\n",
    "    - Write-write conflicts occur when two processes want to perform an update on the same data.\n",
    "\n",
    "#### Partial Quorums\n",
    "\n",
    "- Derivatives of Anamzon's Dynamo, allow various degress of consistency by tuning $N_{R}$ and $N_{W}$.\n",
    "    - **Strong Consistency**: $N_{R} + N_{W} > N$\n",
    "    - **Weak Consistency**: $N_{R} + N_{W} \\le N$\n",
    "- **Important Note 1**: The strong consistency mode does not avoid write-write conflicts.\n",
    "- **Important Note 2**: The weak consistency mode does not avoid read-write conflicts or write-write conflicts.\n",
    "- To resolve write-write conflicts, updates are tagged with timestamps, and resolution policies such as **last-write wins** or **vector clocks** are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eventually-Consistent Replication\n",
    "\n",
    "- A server that receives an update replies with an acknowledgement to the client first, and then propagates the update lazily to the remaining replicas.\n",
    "- If a replica is unreachable, then it can be updated later using an **anti-entropy** mechanism.\n",
    "    - e.g., Replicas may periodically exchange hashes of data to detect discrepancies using Merkle trees.\n",
    "    - e.g., Updates can be timestamped to enable determination of the latest version of a data item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependability Requirements\n",
    "\n",
    "1. **Availability**: The system should operate correctly at any given instant in time.\n",
    "2. **Reliability**: The system should run continuously without interruption.\n",
    "3. **Safety**: Failure of the system should not have catastrophic consequences.\n",
    "4. **Maintainability**: A failed system should be easy to repair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "- **Failure**: When a system cannot fulfill its promises.\n",
    "- **Error**: Part of a system's state that may lead to a failure.\n",
    "- **Fault**: The cause of an error.\n",
    "\n",
    "> A **fault** may lead to an **error**, which may lead to a **failure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Faults\n",
    "\n",
    "1. **Transient**: Occurs once and disappears.\n",
    "2. **Intermittent**: Occurs and vanishes, reappears.\n",
    "3. **Permanent**: Continues to exist until faulty component is replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Failure\n",
    "\n",
    "1. **Crash Failure**: A server halts, but is working correctly until it halts.\n",
    "2. **Omission Failure**: A server fails to respond to incoming requests.\n",
    "    1. *Receive Omission*: A server fails to receive incoming messages.\n",
    "    2. *Send Omission*: A server fails to send outgoing messages.\n",
    "3. **Timing Failure**: A server's response lies outside the specified time interval.\n",
    "4. **Response Failure**: A server's response is incorrect.\n",
    "    1. *Value Failure*: The value of the response is wrong.\n",
    "    2. *State Transition Failure*: The server deviates from the correct flow of control.\n",
    "5. **Arbitrary Failure**: A server may produce arbitrary responses at arbitrary times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Masking by Redundancy\n",
    "\n",
    "- **Information Redundancy**: Extra bits are added to allow recovery from garbled bits.\n",
    "    - e.g., Hamming Code\n",
    "- **Time Redundancy**: An action is performed, and then, if need be, it is performed again.\n",
    "    - e.g., Transactions, Idempotent Requests\n",
    "- **Physical Redundancy**: Extra equipment or processes are added to make it possible for the system as a whole to tolerate the loss of malfunctioning of some parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilience by Process Groups\n",
    "\n",
    "![Flat Groups and Hierarchical Groups](images/Fault_Tolerance_1.png)\n",
    "\n",
    "- Protection against process failures can be achieved by replicating processes into groups.\n",
    "- When a message is sent to the group itself, all members of the group receive it.\n",
    "- The purpose of introducing groups is to alow a process to deal with collections of other processes as a single abstraction.\n",
    "- A **flat group** is symmetrical and has no single point of failure.\n",
    "    - *Advantage*: If one of the processes crashes, the group simply becomes smaller, but can otherwise continue.\n",
    "    - *Disadvantage*: Decision making is more complicated.\n",
    "- A **hierarchical group** is asymmetrical with a single point of failure.\n",
    "    - *Advantage*: Decision making is simpler.\n",
    "    - *Disadvantage*: If the coordinator crashes, the entire group halts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus Problem\n",
    "\n",
    "- Each process has a procedure `propose(val)` and a procedure `decide()`.\n",
    "- Each process first proposes a value by calling `propose(val)` once, with some argument `val` determined by the initial state of the process.\n",
    "- Each process then learns the value agreed upon by calling `decide()`.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "- **Safety Property 1 (Agreement)**: Two calls to `decide()` never return different values.\n",
    "- **Safety Property 2 (Validity)**: If a process calls `decide()` with response `v`, then some process invoked a call to `propose(v)`.\n",
    "- **Liveness Property**: If a process calls `propse(v)` or `decide()` and does not fail, then this call eventually terminates.\n",
    "\n",
    "#### Variations\n",
    "\n",
    "![Circumstances Under Which Distributed Consensus Can Be Reached](images/Fault_Tolerance_2.png)\n",
    "\n",
    "1. **Synchronous vs. Asynchronous Processes**: Is there a bound on the amount of time it takes for a process to take its next step? Is the bound known by all processes?\n",
    "2. **Communication Delays**: Is there a bound on the length of time it takes for a sent message to be delivered? Is the bound known by all processes?\n",
    "3. **Message Delivery Order**: How does the order in which messages are sent affect the order in which they are delivered to the recipients?\n",
    "4. **Unicast vs. Multicast Messaging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPC Failure Semantics\n",
    "\n",
    "1. The client is unable to locate the server.\n",
    "2. The request message from the client to the server is lost.\n",
    "3. The server crashes after receiving a request.\n",
    "4. The reply message from the server to the client is lost.\n",
    "5. The client crashes after sending a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with RPC Server Crashes\n",
    "\n",
    "1. Reissue the request, leading to **at-least-once semantics**. As a side-effect, the request may be processed multiple times by the service handler, which is safe as long as the request is idempotent.\n",
    "2. Give up and report a failure, leading to **at-most-once semantics**. There is no guarantee that the request has been processed.\n",
    "3. Determine whether the request was processed and reissue if needed, leading to **exactly once semantics**. This scheme is difficult to implement as the server may have no way of knowing whether it performed a particular action.\n",
    "4. Make no guarantees at all, leading to confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and Acknowledgments\n",
    "\n",
    "![Actions and Acknowledgements](images/Fault_Tolerance_3.png)\n",
    "\n",
    "- Let $M$ be the server replying to the client with an acknowledgment message.\n",
    "- Let $P$ be the server executing a request from the client.\n",
    "- Let $C$ be the server crashing.\n",
    "\n",
    "\n",
    "1. $M \\to P \\to C$ (*Very Bad*): A crash occurs after sending the completion message and executing the request.\n",
    "2. $M \\to C \\to P$ (*Very Bad*): A crash happens after sending the completion message, but before executing the request.\n",
    "3. $C \\to M \\to P$ (*Bad*): A crash happens before the server could do anything.\n",
    "4. $P \\to M \\to C$ (*Good*): A crash occurs after sending the completion message and executing the request.\n",
    "5. $P \\to C \\to M$ (*Bad*): The request is executed, after which a crash occurs, before the completion message could be send.\n",
    "6. $C \\to P \\to M$ (*Bad*): A crash happens before the server could do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache ZooKeeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- **ZooKeeper** is a centralized system that manages distributed systems as a hierarchical key-value store.\n",
    "- ZooKeeper emphasizes good performance (particularly for read-dominant workloads), being general enough to be used for many different purposes, reliability, and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Use-Cases\n",
    "\n",
    "1. Group Membership\n",
    "2. Leader Election\n",
    "3. Dynamic Configuration\n",
    "4. Status Monitoring\n",
    "5. Queuing\n",
    "6. Barriers\n",
    "7. Critical Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model\n",
    "\n",
    "- **ZooKeeper's Data Model**: A hierchical key-value store similar to a file system.\n",
    "- **ZNode**: A node that may contain data and children.\n",
    "- Reads and writes to a single node are considered to be atomic with values read, or written fully, or not at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Flags\n",
    "\n",
    "1. **Ephemeral Flags**: Make nodes exist as long as the session that created them is active, unless they were explicitly deleted.\n",
    "2. **Sequence Flags**: Make nodes append a monotonically increasing counter to the end of their path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency Model\n",
    "\n",
    "- ZooKeeper ensures that writes are linearizable and that reads are serializable.\n",
    "- ZooKeeper guarantees per-client FIFO servicing of requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Servers\n",
    "\n",
    "- When running in replicated mode, all servers have a copy of the state in memory.\n",
    "- A leader is elected at startup, and all updates go through this leader.\n",
    "- Update responses are sent once a majority of servers have persisted the change.\n",
    "- To tolerate $n$ failures, $2n + 1$ replicated servers are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Commit and Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACID\n",
    "\n",
    "- **Atomicity**: An operation occurs fully or not at all. (*Difficult*)\n",
    "- **Consistency**: A transaction is a valid transformation of the state.\n",
    "- **Isolated**: A transaction is not aware of other concurrent transactions.\n",
    "- **Durable**: Once a transaction completes, its updates persist, even in the event of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Phase Commit (2PC)\n",
    "\n",
    "- **Two-Phase Commit**: A coordinator-based distributed transaction commitment protocol.\n",
    "- **Phase 1**: Coordinator asks participants whether they are ready to commit.\n",
    "- **Phase 2**: Coordinator examines votes and decides the outcome of the transaction.\n",
    "    - If all participants vote to commit, then the transaction is committed successfully.\n",
    "    - Otherwise, the transaction is aborted.\n",
    "    \n",
    "#### Key Assumptions\n",
    "\n",
    "- Synchronous Processes\n",
    "- Bounded Communication Delays\n",
    "- Crash-Recovery Failures\n",
    "- Stable Storage with Recovery Logs\n",
    "\n",
    "#### States and Transitions\n",
    "\n",
    "![2PC States and Transitions](images/2PC_1.png)\n",
    "\n",
    "- *(a): Coordinator*\n",
    "- *(b): Participants*\n",
    "\n",
    "#### Participant-Participant Communication\n",
    "\n",
    "![Participant-Participant Communication](images/2PC_2.png)\n",
    "\n",
    "- If a participant $P$ does not receive a decision from the coordinator within a bounded period of time, it may try to learn the decision from another participant $Q$.\n",
    "\n",
    "#### Coordinator Crashes\n",
    "\n",
    "- A participant is able to make progress as long as it received the decision from the coordinator despite the crash, or if it was able to learn the decision from another participant.\n",
    "- In general, the transaction is safe to commit if all participants voted to commit (*READY* or *COMMIT*), and safe to abort otherwise.\n",
    "\n",
    "#### Simultaneous Coordinator and Participant Crashes\n",
    "\n",
    "- A simultaneous failure of the coordinator and a participant makes it difficult to determine whether all the participants are all *READY*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Checkpoints\n",
    "\n",
    "- Recovery after failure is only possible if the collection of checkpoints by individual processes forms a *distributed snapshot*.\n",
    "- A **distributed snapshot** requires that process checkpoints contain a corresponding send event for each message received.\n",
    "- **Recovery Line**: The most recent distributed snapshot.\n",
    "- **Domino Effect**: If the most recent checkpoints taken by processes do not provide a recovery line, then successively earlier checkpoints must be considered.\n",
    "    - *i.e., Cascading Rollback.*\n",
    "    \n",
    "#### Coordinated Checkpointing Algorithm\n",
    "\n",
    "- The **coordinated checkpointing algorithm** can be applied to create recovery lines.\n",
    "\n",
    "##### Phase 1\n",
    "\n",
    "1. The coordinator sends a *CHECKPOINT_REQUEST* message to all processes.\n",
    "2. Upon receiving a *CHECKPOINT_REQUEST* message, each process does the following:\n",
    "    1. Pause sending new messages to other processes.\n",
    "    2. Takes a local checkpoint.\n",
    "    3. Returns an acknowledgment to the coordinator.\n",
    "\n",
    "##### Phase 2\n",
    "\n",
    "1. Upon receiving acknowledgments from all processes, the coordinator sends a *CHECKPOINT_DONE* message to all processes.\n",
    "2. Upon receiving a *CHECKPOINT_DONE* message, each process resumes processing messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raft Consensus Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicated State Machines\n",
    "\n",
    "- In a **replicated state machine** architecture, a consensus algorithm manages a replicated log containing state machine commands from clients.\n",
    "- The servers' state machines process identical sequences of commands from the logs, so they produce the same outputs.\n",
    "- As a result, the servers appear to form a single, highly reliable state machine.\n",
    "- Furthermore, the distributed system is available as long as any majority of the servers are operational and can communicate with each other and with clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Paxos\n",
    "\n",
    "1. Unintuitive.\n",
    "2. Incomplete.\n",
    "    - Multi-Paxos?\n",
    "    - Liveness?\n",
    "    - Cluster Membership Management?\n",
    "3. Inefficient.\n",
    "    - 2 Message Rounds vs. Single Leader.\n",
    "4. No Agreement on Implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheat Sheet\n",
    "\n",
    "![Raft Consensus Algorithm](images/Raft_Algorithm_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Properties\n",
    "\n",
    "- **Election Safety**: At most one leader can be elected in a given term.\n",
    "- **Leader Append-Only**: A leader never overwrites or deletes entries in its log; it only appends new entries.\n",
    "- **Log Matching**: If two logs contain an entry with the same index and term, then the logs are identical in all entries up though the given index.\n",
    "- **Leader Completeness**: If a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms.\n",
    "- **State Machine Safety**: If a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "![Raft Basics](images/Raft_Algorithm_2.png)\n",
    "\n",
    "#### Server States\n",
    "\n",
    "- A Raft cluster contains several servers in which each server is in one of three states: *leader*, *follower*, and *candidate*.\n",
    "- **Leader**: A single active server that handles all client requests.\n",
    "- **Follower**: Many passive server that respond to requests from leaders and candidates.\n",
    "- **Candidate**: A server used to elect a new leader.\n",
    "\n",
    "#### Terms\n",
    "\n",
    "- **Terms** act as a logical clock that allow servers to detect obsolete information.\n",
    "- Each server stores a current term number, which increases monotonically over time.\n",
    "- Current terms are exchanged whenever servers communicate.\n",
    "- If one server’s current term is smaller than the other’s, then it updates its current term to the larger value.\n",
    "- If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state.\n",
    "- If a server receives a request with a stale term number, it rejects the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leader Election\n",
    "\n",
    "\n",
    "- **Goal**: A new leader must be chosen when an existing leader fails.\n",
    "\n",
    "\n",
    "1. A server starts as a follower and remains a follower as long as it receives periodic, valid RPCs from a leader or a candidate.\n",
    "2. If the follower receives no communication over the **election timeout**, then it begins a leader election.\n",
    "3. The follower increments its current term and becomes a candidate.\n",
    "4. The candidate votes for itself and broadcasts *RequestVote* RPCs in parallel to each of the other servers in the cluster.\n",
    "    1. If the candidate receives a majority of the votes, it becomes the new leader.\n",
    "    2. If the candidate receives an *AppendEntries* RPC from a leader whose term is at least as large as the candidate's current term, the candidate becomes a follower.\n",
    "    3. If the candidate neither wins nor loses the election, the candidate starts a new leader election.\n",
    "    \n",
    "\n",
    "- **Safety**: Each server will vote for at most one candidate in a given term, on a first-come-first-served basis.\n",
    "- **Liveness**: Randomized election timeouts are used to ensure that split votes are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Replication\n",
    "\n",
    "\n",
    "- **Goal**: The leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own.\n",
    "\n",
    "\n",
    "1. The leader receives a command from a client.\n",
    "2. The leader appends the command to its log as a new entry.\n",
    "3. The leader broadcasts *AppendEntries* RPCs in parallel to each of the other servers in the cluster.\n",
    "4. When the entry has been safely replicated, the leader applies the entry applies the entry to its state machine.\n",
    "5. The leader returns the result of the command's execution to the client.\n",
    "\n",
    "\n",
    "- **Important Note**: For crashed or slow followers, the leader retries *AppendEntries* RPCs until they succeed.\n",
    "\n",
    "#### Log Structure\n",
    "\n",
    "![Raft Log Structure](images/Raft_Algorithm_3.png)\n",
    "\n",
    "- **Committed**: A durable entry guaranteed to be executed by all of the available state machines if it has been replicated on a majority of servers.\n",
    "\n",
    "#### Log Matching Property\n",
    "\n",
    "1. If two entries in different logs have the same index and term, then they store the same command.\n",
    "2. If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\n",
    "\n",
    "#### *AppendEntries* Consistency Check (Induction Step)\n",
    "\n",
    "- When sending an *AppendEntries* RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries.\n",
    "- If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.\n",
    "- Inconsistencies are handled by forcing the followers' logs to duplicate the leader's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety\n",
    "\n",
    "- **Goal**: If any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.\n",
    "\n",
    "#### Leader Completeness Property\n",
    "\n",
    "- **Leader Completeness Property**: Once a log entry is committed, all future leaders must store that entry.\n",
    "- Therefore, servers with incomplete logs must not get elected.\n",
    "    - Candidates include the index and the term of the last log entry in their *RequestVote* RPCs.\n",
    "    - Followers denies a *RequestVote* RPC, if their logs are more up-to-date than the RPC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "text/x-java",
   "file_extension": ".java",
   "mimetype": "",
   "name": "Java",
   "nbconverter_exporter": "",
   "version": "11.0.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
