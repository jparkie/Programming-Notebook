{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 493 - Probabilistic Reasoning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *See [Stanford CS228 - Probability Review](https://ermongroup.github.io/cs228-notes/preliminaries/probabilityreview/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem**: *How do we express a probability distribution $p(x_{1}, x_{2}, ..., x_{n})$ that models some real-world phenomenon?*\n",
    "    - *Naive Complexity*: $O(d^{n})$\n",
    "- **Solution**: *Representation with Probabilistic Graphical Models + Verifying Independence Assumptions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks (Directed Probabilistic Graphical Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition - What is a Bayesian network?\n",
    "\n",
    "- A **Bayesian network** is a directed graph $G$ with the following:\n",
    "    - *Nodes*: A random variable $x_{i}$.\n",
    "    - *Edges*: A conditional probability distribution (CPD) $p(x_{i} \\mid x_{A_{i}})$ per node, specifying the probability of $x_i$ conditioned on its parent's values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation - How does a Bayesian network express a probability distribution?\n",
    "\n",
    "1. Let $p$ be a probability distribution.\n",
    "2. A naive representation of $p$ can be derived using the chain rule:\n",
    "$$p(x_{1}, x_{2}, ..., x_{n}) = p(x_{1}) p(x_{2} \\mid x_{1}) \\cdots p(x_{n} \\mid x_{n - 1}, ..., x_{2}, x_{1})$$\n",
    "3. A Bayesian network representation of $p$ compacts the naive representation by having each factor in the right hand side depend only on a small number of **ancestor variables** $x_{A_{i}}$:\n",
    "$$p(x_{i} \\mid x_{i - 1}, ..., x_{2}, x_{1}) = p(x_{i} \\mid x_{A_{i}})$$\n",
    "    - e.g., Approximate $p(x_{5} \\mid x_{4}, x_{3}, x_{2}, x_{1})$ with $p(x_{5} \\mid x_{A_{5}})$ where $x_{A_{5}} = \\{x_{4}, x_{3}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space Complexity - How compact is a Bayesian network?\n",
    "\n",
    "- Consider each of the factors $p(x_{i} \\mid x_{A_{i}})$ as a **probability table**:\n",
    "    - *Rows*: Values of $x_{i}$\n",
    "    - *Columns*: Values of $x_{A_{i}}$\n",
    "    - *Cells*: Values of $p(x_{i} \\mid x_{A_{i}})$\n",
    "- If each variable takes $d$ values and has at most $k$ ancestors, then each probability table has at most $O(d^{k + 1})$ entries.\n",
    "- **Naive Representation Space Complexity**: $O(d^n)$\n",
    "- **Bayesian Networks Representation Space Complexity**: $O(n \\cdot d^{k + 1})$\n",
    "$$\\approx \\text{Bayesian Networks Representation} \\le \\text{Naive Representation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence Assumptions - Why are the independence assumptions of a Bayesian network important to identify?\n",
    "\n",
    "- A Bayesian network expresses a probability distribution $p$ via products of smaller, local conditional probability distributions (one for each variable).\n",
    "- These smaller, local conditional probability distributions introduces assumptions into the model of $p$ that certain variables are independent.\n",
    "- **Important Note**: Which independence assumptions are we exactly making by using a Bayesian network?\n",
    "    - *Correctness: Are these independence assumptions correct?*\n",
    "    - *Efficiency: Do these independence assumptions efficiently compact the representation?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $3$-Variable Independencies in Directed Graphs - How do you identify independent variables in a $3$-variable Bayesian network?\n",
    "\n",
    "- Let $x \\perp y$ indicate that variables $x$ and $y$ are independent.\n",
    "- Let $G$ be a Bayesian network with three nodes: $A$, $B$, and $C$.\n",
    "\n",
    "##### Common Parent\n",
    "\n",
    "- If $G$ is of the form $A \\leftarrow B \\rightarrow C$,\n",
    "    - If $B$ is observed, then $A \\perp C \\mid B$\n",
    "    - If $B$ is unobserved, then $A \\not\\perp C$\n",
    "- **Intuition**: $B$ contains all the information that determines the outcomes of $A$ and $C$; once it is observed, there is nothing else that affects $A$'s and $C$s' outcomes.\n",
    "\n",
    "##### Cascade\n",
    "\n",
    "- If $G$ equals $A \\rightarrow B \\rightarrow C$,\n",
    "    - If $B$ is observed, then $A \\perp C \\mid B$\n",
    "    - If $B$ is unobserved, then $A \\not\\perp C$\n",
    "- **Intuition**: $B$ contains all the information that determines the outcomes of $C$; once it is observed, there is nothing else that affects $C$'s outcomes.\n",
    "\n",
    "##### V-Structure\n",
    "\n",
    "- If $G$ is $A \\rightarrow C \\leftarrow B$, then knowing $C$ couples $A$ and $B$.\n",
    "    - If $C$ is unobserved, then $A \\perp B$\n",
    "    - If $C$ is observed, then $A \\not\\perp B \\mid C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $n$-Variable Independencies in Directed Graphs - How do you identify independent variables in a $n$-variable Bayesian network?\n",
    "\n",
    "- Let $I(p)$ be the set of all independencies that hold for a probability distribution $p$.\n",
    "- Let $I(G) = \\{(X \\perp Y \\mid Z) : X, Y \\text{ are } d\\text{-sep given } Z\\}$ be a set of variables that are $d$-separated in $G$.\n",
    "- If the probability distribution $p$ factorizes over $G$, then $I(G) \\subseteq I(p)$ and $G$ is an $I$-map (**independence map**) for $p$.\n",
    "- **Important Note 1**: Thus, variables that are $d$-separated in $G$ are independent in $p$.\n",
    "- **Important Note 2**: However, a probability distribution $q$ can factorize over $G$, yet have independencies that are not captured in $G$.\n",
    "- **Important Caveat**: A Bayesian network cannot perfectly represent all probability distributions.\n",
    "\n",
    "##### $d$-separation (a.k.a. Directed Separation)\n",
    "\n",
    "- $Q$ and $W$ are **$d$-separated** when variables $O$ are observed if they are **NOT CONNECTED** by an active path.\n",
    "\n",
    "##### Active Path\n",
    "\n",
    "- An undirected path in the Bayesian Network structure $G$ is called **active** given observed variables $O$ if for **EVERY CONSECUTIVE TRIPLE** of variables $X$, $Y$, $Z$ on the path, one of the following holds:\n",
    "    - **Evidential Trail**: $X \\leftarrow Y \\leftarrow Z$, and $Y$ is unobserved $Y \\not\\in O$\n",
    "    - **Causal Trail**: $X \\rightarrow Y \\rightarrow Z$, and $Y$ is unobserved $Y \\not\\in O$\n",
    "    - **Common Cause**: $X \\leftarrow Y \\rightarrow Z$, and $Y$ is unobserved $Y \\not\\in O$\n",
    "    - **Common Effect**: $X \\rightarrow Y \\leftarrow Z$, and $Y$ or any of its descendants are observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalence - When are two Bayesian networks $I$-equivalent?\n",
    "\n",
    "- $G_1$ and $G_2$ are **$I$-equivalent**...\n",
    "    - If they encode the same dependencies: $I(G_1) = I(G_2)$.\n",
    "    - If they have the same skeleton and the same v-structures.\n",
    "    - If the $d$-separation between variables is the same.\n",
    "    \n",
    "##### Skeleton\n",
    "\n",
    "![Skeleton](images/BN_1.png)\n",
    "\n",
    "- A **skeleton** is an undirected graph obtained by dropping the directionality of the arrows.\n",
    "    - (a) is Cascade\n",
    "    - (b) is Cascade\n",
    "    - (c) is Common Parent\n",
    "    - (d) is V-Structure\n",
    "    - (a), (b), (c), and (d) have the same skeleton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Problem 1 - $d$-separation\n",
    "\n",
    "![Problem 1](images/BN_P1.png)\n",
    "\n",
    "##### Question\n",
    "\n",
    "- Are $X_{1}$ and $X_{6}$ $d$-separated given $\\{X_{2}, X_{3}\\}$?\n",
    "\n",
    "##### Solution\n",
    "\n",
    "1. **Path**: $X_{1} \\rightarrow X_{2} \\rightarrow X_{6}$\n",
    "    1. *Consecutive Triple*: $X_{1} \\rightarrow X_{2} \\rightarrow X_{6}$\n",
    "        - Although $X_{2}$ is observed, the *common effect* does not hold.\n",
    "    2. As not all the consecutive triples hold, this path is not *active*.\n",
    "2. **Path**: $X_{1} \\rightarrow X_{3} \\rightarrow X_{5} \\rightarrow X_{6}$\n",
    "    1. *Consecutive Triple*: $X_{1} \\rightarrow X_{3} \\rightarrow X_{5}$\n",
    "        - Although $X_{3}$ is observed, the *common effect* does not hold.\n",
    "    2. *Consecutive Triple*: $X_{3} \\rightarrow X_{5} \\rightarrow X_{6}$\n",
    "        - As $X_{5}$ is unobserved, the *causal trail* does hold.\n",
    "    3. As not all the consecutive triples hold, this path is not *active*.\n",
    "3. As there are no active paths between $X_{1}$ and $X_{6}$, they are $d$-separated given $\\{X_{2}, X_{3}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Problem 2 - $d$-separation\n",
    "\n",
    "![Problem 2](images/BN_P2.png)\n",
    "\n",
    "##### Question\n",
    "\n",
    "- Are $X_{2}$ and $X_{3}$ $d$-separated given $\\{X_{1}, X_{6}\\}$?\n",
    "\n",
    "##### Solution\n",
    "\n",
    "1. **Path**: $X_{2} \\leftarrow X_{1} \\rightarrow X_{3}$\n",
    "    1. *Consecutive Triple*: $X_{2} \\leftarrow X_{1} \\rightarrow X_{3}$\n",
    "        - Although $X_{1}$ is observed, the *common effect* does not hold.\n",
    "    2. As not all the consecutive triples hold, this path is not *active*.\n",
    "2. **Path**: $X_{2} \\rightarrow X_{6} \\leftarrow X_{5} \\leftarrow X_{3}$\n",
    "    1. *Consecutive Triple*: $X_{2} \\rightarrow X_{6} \\leftarrow X_{5}$\n",
    "        - As $X_{6}$ is observed, the *common effect* does hold.\n",
    "    2. *Consecutive Triple*: $X_{6} \\leftarrow X_{5} \\leftarrow X_{3}$\n",
    "        - As $X_{5}$ is unobserved. the *causal trail* does hold.\n",
    "    3. As all the consecutive triples hold, this path is *active*.\n",
    "3. As there exists an active path between $X_{2}$ and $X_{3}$, they are not $d$-separated given $\\{X_{1}, X_{6}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Random Fields (Undirected Probabilistic Graphical Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition - What is a Markov random field?\n",
    "\n",
    "- A **Markov random field** is an undirected graph $G$ with the following:\n",
    "    - *Nodes*: A random variable $x_{i}$.\n",
    "    - *Fully Connected Subgraphs*: An optional factor $\\phi_{c}(x_{c})$ per clique, specifying the level of coupling (**potentials**) between all the dependent variables within the clique.\n",
    "- **Important Note**:\n",
    ">...SPECIFYING THE LEVEL OF COUPLING BETWEEN ALL THE DEPENDENT VARIABLES WITHIN THE CLIQUE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation - How does a Markov random field express a probability distribution?\n",
    "\n",
    "1. Let $p$ be a probability distribution.\n",
    "2. A Markov random field representation of $p$ is the following:\n",
    "$$p(x_{1}, x_{2}, ..., x_{n}) = \\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}(x_{c})$$\n",
    "    - Where $C$ is the set of cliques of $G$.\n",
    "    - Where $\\phi_{c}$ is a **factor** (nonnegative function) over the variables in a clique.\n",
    "    - Where $Z$ is a **normalizing constant** that ensures that $p$ sums to one.\n",
    "$$Z = \\sum_{x_{1}, x_{2}, ..., x_{n}} \\prod_{c \\in C} \\phi_{c}(x_{c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space Complexity - How compact is a Markov random field?\n",
    "\n",
    "##### Factor Product\n",
    "\n",
    "- Let $A$, $B$, and $C$ be three disjoint sets of variables.\n",
    "- Let $\\phi_{1}(A, B)$ and $\\phi_{2}(B, C)$ be two factors.\n",
    "- Let $\\psi(A, B, C)$ be the factor product $\\phi_{1} \\times \\phi_{2}$.\n",
    "$$\\psi(A, B, C) = \\phi_{1}(A, B) \\cdot \\phi_{2}(B, C)$$\n",
    "    - Where the two factors are multiplied for common values of $B$.\n",
    "\n",
    "##### Binary Factor Tables\n",
    "\n",
    "- Each of the optional factors $\\phi_{c}(x_{c})$ can be expressed as a product of **binary factor tables** $\\phi(X, Y)$:\n",
    "    - *Rows*: Values of $X$\n",
    "    - *Columns*: Values of $Y$\n",
    "    - *Cells*: Values of $\\phi(X, Y)$\n",
    "- If each variable takes $d$ values, each binary factor table has at most $O(d^{2})$ entries.\n",
    "- **Markov Random Fields Representation Space Complexity**: $O(E \\cdot d^{2})$\n",
    "    - Where $E$ is the number of edges in a Markov random field.\n",
    "$$\\approx \\text{Markov Random Field Representation} \\le \\text{Naive Representation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Random Fields vs. Bayesian Networks - What are the advantages and disadvantages of Markov random fields?\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "- *Applicable for Variable Dependencies Without Natural Directionality*\n",
    "- *Succinctly Express Dependencies Not Easily Expressible in Bayesian Networks*\n",
    "\n",
    "##### Disadvantages\n",
    "\n",
    "- *Cannot Express Dependencies Easily Expressible in Bayesian Networks*\n",
    "    - *e.g., V-Structures*\n",
    "- *Computing Normalization Constant $Z$ Is NP-Hard*\n",
    "- *Generally Require Approximation Techniques*\n",
    "- *Difficult to Interpret*\n",
    "- *Easier to Construct Bayesian Networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moralization - What is moralization?\n",
    "\n",
    "![Moralization](images/MRF_1.png)\n",
    "\n",
    "- Bayesian networks are a special case of Markov random fields with factors corresponding to conditional probability distributions and a normalizing constant of one.\n",
    "- **Moralization**: Bayesian Network $\\to$ Markov Random Field\n",
    "    1. Add side edges to all parents of a given node.\n",
    "    2. Remove the directionality of all the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $n$-Variable Independencies in Undirected Graphs - How do you identify independent variables in a $n$-variable Markov random field?\n",
    "\n",
    "1. If variables $X$ and $Y$ are connected by a path of unobserved variables, then $X$ and $Y$ are dependent.\n",
    "2. If variable $X$'s neighbors are all observed, then $X$ is independent of all the other variables.\n",
    "3. If a set of observed variables forms a cut-set between two halves of the graph, then variables in one half are independent from ones in the other.\n",
    "\n",
    "##### Cut-Set Variable Independencies\n",
    "\n",
    "![Cut-Set Variable Independencies](images/MRF_2.png)\n",
    "\n",
    "##### Markov Blanket\n",
    "\n",
    "- The **Markov blanket** $U$ of a variable $X$ is the minimal set of nodes such that $X$ is independent from the rest of the graph if $U$ is observed.\n",
    "$$X \\perp (\\mathcal{X} - \\{X\\} - U) \\mid U$$\n",
    "- In an undirected graph, the Markov blanket is a node's neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Random Fields - What are conditional random fields?\n",
    "\n",
    "##### Definition\n",
    "\n",
    "- A **conditional random field** is a Markov random field over variables $\\mathcal{X} \\cup \\mathcal{Y}$ which specifies a conditional distribution:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y \\mid x) &= \\frac{1}{Z(x)} \\prod_{c \\in C} \\phi_{c}(x_{c}, y_{c}) \\\\\n",
    "Z(x) &= \\sum_{y \\in \\mathcal{Y}} \\prod_{c \\in C} \\phi_{c}(x_{c}, y_{c})\n",
    "\\end{align}\n",
    "$$\n",
    "    - Where $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$ are **VECTOR-VALUED** variables.\n",
    "    - Where $Z(x)$ is the partition function.\n",
    "- **Important Note 1**: A conditional random field results in an instantiation of a new Markov random field for each input $x$.\n",
    "- **Important Note 2**: A conditional random field is useful for structured prediction in which the output labels are predicted considering the neighboring input samples.\n",
    "    - *See [Stanford CS228 - Markov Random Fields: Conditional Random Fields (OCR Example)](https://ermongroup.github.io/cs228-notes/representation/undirected/#conditional-random-fields).*\n",
    "\n",
    "##### Features\n",
    "\n",
    "- Assume the factors $\\phi_{c}(x_{c}, y_{c})$ are of the following form:\n",
    "$$\\phi_{c}(x_{c}, y_{c}) = \\exp(w_{c}^{T} f_{c}(x_{c}, y_{c}))$$\n",
    "    - Where $f_{c}(x_{c}, y_{c})$ can be an arbitrary set of features describing the compatibility between $x_{c}$ and $y_{c}$.\n",
    "    - Where $w_{c}^{T}$ is the transposed weight matrix.\n",
    "- Accordingly, $f_{c}(x_{c}, y_{c})$ allows arbitrarily complex features.\n",
    "    - e.g., $f(x, y_{i})$ are features that depend on the entirety of input samples $x$.\n",
    "    - e.g., $f(y_{i}, y_{i + 1})$ are features that depend on successive pairs of output labels $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditonal Random Fields vs. Markov Random Fields - Why is a conditional random field a special case of Markov random fields?\n",
    "\n",
    "- If we were to model $p(x, y)$ using a Markov random field, then we need to fit two probability distributions to the data: $p(y \\mid x)$ and $p(x)$.\n",
    "    - *Remember Baye's Rule*: $p(x, y) = p(y \\mid x) \\cdot p(x)$\n",
    "- However, if all we are interested in is predicting $y$ given $x$, then modeling $p(x)$ is expensive and unnecessary.\n",
    "$$\\text{Prediction} \\implies \\text{CRF} > \\text{MRF}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor Graphs - What is a factor graph? Why does a factor graph exist?\n",
    "\n",
    "![Factor Graph](images/MRF_3.png)\n",
    "\n",
    "- A **factor graph** is a bipartite graph where one group is the variables in the distribution being modeled, and the other group is the factors defined on these variables.\n",
    "    - *Edges Between Factors and Variables*\n",
    "- **Side Note**: A **bipartite graph** is a graph whose vertices are divided into two disjoint and independent sets.\n",
    "    - *Set 1: Variables*\n",
    "    - *Set 2: Factors*\n",
    "- **Important Note**: Use a factor graph to identify what variables a factor depends on when computing probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "- **Problem**: *Given a probabilistic model, how do we obtain answers to relevant questions about the world?*\n",
    "    - **Marginal Inference**: *What is the probability of a given variable in our model after we sum everything else out?*\n",
    "    - **Maximum A Posteriori**: *What is the most likely assignment of variables?*\n",
    "    - *Naive Complexity*: NP-Hard\n",
    "- **Solution**: TODO."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
