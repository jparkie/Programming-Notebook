{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 493 - Probabilistic Reasoning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Probability Theory <span name=\"S01\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *See [Stanford CS228 - Probability Review](https://ermongroup.github.io/cs228-notes/preliminaries/probabilityreview/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation <span name=\"S02\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem**: *How do we express a probability distribution $p(x_{1}, x_{2}, ..., x_{n})$ that models some real-world phenomenon?*\n",
    "    - *Naive Complexity*: $O(d^{n})$\n",
    "- **Solution**: *Representation with Probabilistic Graphical Models + Verifying Independence Assumptions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks (Directed Probabilistic Graphical Model) <span name=\"S02-1\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition - What is a Bayesian network?\n",
    "\n",
    "- A **Bayesian network** is a directed graph $G$ with the following:\n",
    "    - *Nodes*: A random variable $x_{i}$.\n",
    "    - *Edges*: A conditional probability distribution (CPD) $p(x_{i} \\mid x_{A_{i}})$ per node, specifying the probability of $x_i$ conditioned on its parent's values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation - How does a Bayesian network express a probability distribution?\n",
    "\n",
    "1. Let $p$ be a probability distribution.\n",
    "2. A naive representation of $p$ can be derived using the chain rule:\n",
    "$$p(x_{1}, x_{2}, ..., x_{n}) = p(x_{1}) p(x_{2} \\mid x_{1}) \\cdots p(x_{n} \\mid x_{n - 1}, ..., x_{2}, x_{1})$$\n",
    "3. A Bayesian network representation of $p$ compacts the naive representation by having each factor in the right hand side depend only on a small number of **ancestor variables** $x_{A_{i}}$:\n",
    "$$p(x_{i} \\mid x_{i - 1}, ..., x_{2}, x_{1}) = p(x_{i} \\mid x_{A_{i}})$$\n",
    "    - e.g., Approximate $p(x_{5} \\mid x_{4}, x_{3}, x_{2}, x_{1})$ with $p(x_{5} \\mid x_{A_{5}})$ where $x_{A_{5}} = \\{x_{4}, x_{3}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space Complexity - How compact is a Bayesian network?\n",
    "\n",
    "- Consider each of the factors $p(x_{i} \\mid x_{A_{i}})$ as a **probability table**:\n",
    "    - *Rows*: Values of $x_{i}$\n",
    "    - *Columns*: Values of $x_{A_{i}}$\n",
    "    - *Cells*: Values of $p(x_{i} \\mid x_{A_{i}})$\n",
    "- If each discrete random variable takes $d$ possible values and has at most $k$ ancestors, then each probability table has at most $O(d^{k + 1})$ entries.\n",
    "- **Naive Representation Space Complexity**: $O(d^n)$\n",
    "- **Bayesian Networks Representation Space Complexity**: $O(n \\cdot d^{k + 1})$\n",
    "$$\\approx \\text{Bayesian Networks Representation} \\le \\text{Naive Representation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence Assumptions - Why are the independence assumptions of a Bayesian network important to identify?\n",
    "\n",
    "- A Bayesian network expresses a probability distribution $p$ via products of smaller, local conditional probability distributions (one for each variable).\n",
    "- These smaller, local conditional probability distributions introduces assumptions into the model of $p$ that certain variables are independent.\n",
    "- **Important Note**: Which independence assumptions are we exactly making by using a Bayesian network?\n",
    "    - *Correctness: Are these independence assumptions correct?*\n",
    "    - *Efficiency: Do these independence assumptions efficiently compact the representation?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $3$-Variable Independencies in Directed Graphs - How do you identify independent variables in a $3$-variable Bayesian network?\n",
    "\n",
    "- Let $x \\perp y$ indicate that variables $x$ and $y$ are independent.\n",
    "- Let $G$ be a Bayesian network with three nodes: $A$, $B$, and $C$.\n",
    "\n",
    "##### Common Parent\n",
    "\n",
    "- If $G$ is of the form $A \\leftarrow B \\rightarrow C$,\n",
    "    - If $B$ is observed, then $A \\perp C \\mid B$\n",
    "    - If $B$ is unobserved, then $A \\not\\perp C$\n",
    "- **Intuition**: $B$ contains all the information that determines the outcomes of $A$ and $C$; once it is observed, there is nothing else that affects $A$'s and $C$s' outcomes.\n",
    "\n",
    "##### Cascade\n",
    "\n",
    "- If $G$ equals $A \\rightarrow B \\rightarrow C$,\n",
    "    - If $B$ is observed, then $A \\perp C \\mid B$\n",
    "    - If $B$ is unobserved, then $A \\not\\perp C$\n",
    "- **Intuition**: $B$ contains all the information that determines the outcomes of $C$; once it is observed, there is nothing else that affects $C$'s outcomes.\n",
    "\n",
    "##### V-Structure\n",
    "\n",
    "- If $G$ is $A \\rightarrow C \\leftarrow B$, then knowing $C$ couples $A$ and $B$.\n",
    "    - If $C$ is unobserved, then $A \\perp B$\n",
    "    - If $C$ is observed, then $A \\not\\perp B \\mid C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $n$-Variable Independencies in Directed Graphs - How do you identify independent variables in a $n$-variable Bayesian network?\n",
    "\n",
    "- Let $I(p)$ be the set of all independencies that hold for a probability distribution $p$.\n",
    "- Let $I(G) = \\{(X \\perp Y \\mid Z) : X, Y \\text{ are } d\\text{-sep given } Z\\}$ be a set of variables that are $d$-separated in $G$.\n",
    "- If the probability distribution $p$ factorizes over $G$, then $I(G) \\subseteq I(p)$ and $G$ is an $I$-map (**independence map**) for $p$.\n",
    "- **Important Note 1**: Thus, variables that are $d$-separated in $G$ are independent in $p$.\n",
    "- **Important Note 2**: However, a probability distribution $q$ can factorize over $G$, yet have independencies that are not captured in $G$.\n",
    "- **Important Caveat**: A Bayesian network cannot perfectly represent all probability distributions.\n",
    "\n",
    "##### $d$-separation (a.k.a. Directed Separation)\n",
    "\n",
    "- $Q$ and $W$ are **$d$-separated** when variables $O$ are observed if they are **NOT CONNECTED** by an active path.\n",
    "\n",
    "##### Active Path\n",
    "\n",
    "- An undirected path in the Bayesian Network structure $G$ is called **active** given observed variables $O$ if for **EVERY CONSECUTIVE TRIPLE** of variables $X$, $Y$, $Z$ on the path, one of the following holds:\n",
    "    - **Evidential Trail**: $X \\leftarrow Y \\leftarrow Z$, and $Y$ is unobserved $Y \\not\\in O$\n",
    "    - **Causal Trail**: $X \\rightarrow Y \\rightarrow Z$, and $Y$ is unobserved $Y \\not\\in O$\n",
    "    - **Common Cause**: $X \\leftarrow Y \\rightarrow Z$, and $Y$ is unobserved $Y \\not\\in O$\n",
    "    - **Common Effect**: $X \\rightarrow Y \\leftarrow Z$, and $Y$ or any of its descendants are observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalence - When are two Bayesian networks $I$-equivalent?\n",
    "\n",
    "- $G_1$ and $G_2$ are **$I$-equivalent**...\n",
    "    - If they encode the same dependencies: $I(G_1) = I(G_2)$.\n",
    "    - If they have the same skeleton and the same v-structures.\n",
    "    - If the $d$-separation between variables is the same.\n",
    "    \n",
    "##### Skeleton\n",
    "\n",
    "![Skeleton](images/BN_1.png)\n",
    "\n",
    "- A **skeleton** is an undirected graph obtained by dropping the directionality of the arrows.\n",
    "    - (a) is Cascade\n",
    "    - (b) is Cascade\n",
    "    - (c) is Common Parent\n",
    "    - (d) is V-Structure\n",
    "    - (a), (b), (c), and (d) have the same skeleton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Problem 1 - $d$-separation\n",
    "\n",
    "![Problem 1 - $d$-separation](images/BN_P1.png)\n",
    "\n",
    "##### Question\n",
    "\n",
    "- Are $X_{1}$ and $X_{6}$ $d$-separated given $\\{X_{2}, X_{3}\\}$?\n",
    "\n",
    "##### Solution\n",
    "\n",
    "1. **Path**: $X_{1} \\rightarrow X_{2} \\rightarrow X_{6}$\n",
    "    1. *Consecutive Triple*: $X_{1} \\rightarrow X_{2} \\rightarrow X_{6}$\n",
    "        - Although $X_{2}$ is observed, the *common effect* does not hold.\n",
    "    2. As not all the consecutive triples hold, this path is not *active*.\n",
    "2. **Path**: $X_{1} \\rightarrow X_{3} \\rightarrow X_{5} \\rightarrow X_{6}$\n",
    "    1. *Consecutive Triple*: $X_{1} \\rightarrow X_{3} \\rightarrow X_{5}$\n",
    "        - Although $X_{3}$ is observed, the *common effect* does not hold.\n",
    "    2. *Consecutive Triple*: $X_{3} \\rightarrow X_{5} \\rightarrow X_{6}$\n",
    "        - As $X_{5}$ is unobserved, the *causal trail* does hold.\n",
    "    3. As not all the consecutive triples hold, this path is not *active*.\n",
    "3. As there are no active paths between $X_{1}$ and $X_{6}$, they are $d$-separated given $\\{X_{2}, X_{3}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Problem 2 - $d$-separation\n",
    "\n",
    "![Problem 2 - $d$-separation](images/BN_P2.png)\n",
    "\n",
    "##### Question\n",
    "\n",
    "- Are $X_{2}$ and $X_{3}$ $d$-separated given $\\{X_{1}, X_{6}\\}$?\n",
    "\n",
    "##### Solution\n",
    "\n",
    "1. **Path**: $X_{2} \\leftarrow X_{1} \\rightarrow X_{3}$\n",
    "    1. *Consecutive Triple*: $X_{2} \\leftarrow X_{1} \\rightarrow X_{3}$\n",
    "        - Although $X_{1}$ is observed, the *common effect* does not hold.\n",
    "    2. As not all the consecutive triples hold, this path is not *active*.\n",
    "2. **Path**: $X_{2} \\rightarrow X_{6} \\leftarrow X_{5} \\leftarrow X_{3}$\n",
    "    1. *Consecutive Triple*: $X_{2} \\rightarrow X_{6} \\leftarrow X_{5}$\n",
    "        - As $X_{6}$ is observed, the *common effect* does hold.\n",
    "    2. *Consecutive Triple*: $X_{6} \\leftarrow X_{5} \\leftarrow X_{3}$\n",
    "        - As $X_{5}$ is unobserved. the *causal trail* does hold.\n",
    "    3. As all the consecutive triples hold, this path is *active*.\n",
    "3. As there exists an active path between $X_{2}$ and $X_{3}$, they are not $d$-separated given $\\{X_{1}, X_{6}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Random Fields (Undirected Probabilistic Graphical Model) <span name=\"S02-2\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition - What is a Markov random field?\n",
    "\n",
    "- A **Markov random field** is an undirected graph $G$ with the following:\n",
    "    - *Nodes*: A random variable $x_{i}$.\n",
    "    - *Fully Connected Subgraphs*: An optional factor $\\phi_{c}(x_{c})$ per clique, specifying the level of coupling (**potentials**) between all the dependent variables within the clique.\n",
    "- **Important Note**:\n",
    ">...SPECIFYING THE LEVEL OF COUPLING BETWEEN ALL THE DEPENDENT VARIABLES WITHIN THE CLIQUE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation - How does a Markov random field express a probability distribution?\n",
    "\n",
    "1. Let $p$ be a probability distribution.\n",
    "2. A Markov random field representation of $p$ is the following:\n",
    "$$p(x_{1}, x_{2}, ..., x_{n}) = \\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}(x_{c})$$\n",
    "    - Where $C$ is the set of cliques of $G$.\n",
    "    - Where $\\phi_{c}$ is a **factor** (nonnegative function) over the variables in a clique.\n",
    "    - Where $Z$ is a **normalizing constant** that ensures that $p$ sums to one.\n",
    "$$Z = \\sum_{x_{1}, x_{2}, ..., x_{n}} \\prod_{c \\in C} \\phi_{c}(x_{c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space Complexity - How compact is a Markov random field?\n",
    "\n",
    "##### Factor Product\n",
    "\n",
    "- Let $A$, $B$, and $C$ be three disjoint sets of variables.\n",
    "- Let $\\phi_{1}(A, B)$ and $\\phi_{2}(B, C)$ be two factors.\n",
    "- Let $\\phi_{3}(A, B, C)$ be the **factor product**.\n",
    "$$\\phi_{3}(A, B, C) = \\phi_{1}(A, B) \\cdot \\phi_{2}(B, C)$$\n",
    "    - Where the two factors are multiplied for common values of $B$.\n",
    "\n",
    "##### Binary Factor Tables\n",
    "\n",
    "- Each of the optional factors $\\phi_{c}(x_{c})$ can be expressed as a product of **binary factor tables** $\\phi(X, Y)$:\n",
    "    - *Rows*: Values of $X$\n",
    "    - *Columns*: Values of $Y$\n",
    "    - *Cells*: Values of $\\phi(X, Y)$\n",
    "- If each variable takes $d$ values, each binary factor table has at most $O(d^{2})$ entries.\n",
    "- **Markov Random Fields Representation Space Complexity**: $O(E \\cdot d^{2})$\n",
    "    - Where $E$ is the number of edges in a Markov random field.\n",
    "$$\\approx \\text{Markov Random Field Representation} \\le \\text{Naive Representation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Random Fields vs. Bayesian Networks - What are the advantages and disadvantages of Markov random fields?\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "- *Applicable for Variable Dependencies Without Natural Directionality*\n",
    "- *Succinctly Express Dependencies Not Easily Expressible in Bayesian Networks*\n",
    "\n",
    "##### Disadvantages\n",
    "\n",
    "- *Cannot Express Dependencies Easily Expressible in Bayesian Networks*\n",
    "    - *e.g., V-Structures*\n",
    "- *Computing Normalization Constant $Z$ Is NP-Hard*\n",
    "- *Generally Require Approximation Techniques*\n",
    "- *Difficult to Interpret*\n",
    "- *Easier to Construct Bayesian Networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moralization - What is moralization?\n",
    "\n",
    "![Moralization](images/MRF_1.png)\n",
    "\n",
    "- Bayesian networks are a special case of Markov random fields with factors corresponding to conditional probability distributions and a normalizing constant of one.\n",
    "- **Moralization**: Bayesian Network $\\to$ Markov Random Field\n",
    "    1. Add side edges to all parents of a given node.\n",
    "    2. Remove the directionality of all the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $n$-Variable Independencies in Undirected Graphs - How do you identify independent variables in a $n$-variable Markov random field?\n",
    "\n",
    "1. If variables $X$ and $Y$ are connected by a path of unobserved variables, then $X$ and $Y$ are dependent.\n",
    "2. If variable $X$'s neighbors are all observed, then $X$ is independent of all the other variables.\n",
    "3. If a set of observed variables forms a cut-set between two halves of the graph, then variables in one half are independent from ones in the other.\n",
    "\n",
    "##### Cut-Set Variable Independencies\n",
    "\n",
    "![Cut-Set Variable Independencies](images/MRF_2.png)\n",
    "\n",
    "##### Markov Blanket\n",
    "\n",
    "- The **Markov blanket** $U$ of a variable $X$ is the minimal set of nodes such that $X$ is independent from the rest of the graph if $U$ is observed.\n",
    "$$X \\perp (\\mathcal{X} - \\{X\\} - U) \\mid U$$\n",
    "- In an undirected graph, the Markov blanket is a node's neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Random Fields - What are conditional random fields?\n",
    "\n",
    "##### Definition\n",
    "\n",
    "- A **conditional random field** is a Markov random field over variables $\\mathcal{X} \\cup \\mathcal{Y}$ which specifies a conditional distribution:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y \\mid x) &= \\frac{1}{Z(x)} \\prod_{c \\in C} \\phi_{c}(x_{c}, y_{c}) \\\\\n",
    "Z(x) &= \\sum_{y \\in \\mathcal{Y}} \\prod_{c \\in C} \\phi_{c}(x_{c}, y_{c})\n",
    "\\end{align}\n",
    "$$\n",
    "    - Where $x \\in \\mathcal{X}$ and $y \\in \\mathcal{Y}$ are **VECTOR-VALUED** variables.\n",
    "    - Where $Z(x)$ is the partition function.\n",
    "- **Important Note 1**: A conditional random field results in an instantiation of a new Markov random field for each input $x$.\n",
    "- **Important Note 2**: A conditional random field is useful for structured prediction in which the output labels are predicted considering the neighboring input samples.\n",
    "    - *See [Stanford CS228 - Markov Random Fields: Conditional Random Fields (OCR Example)](https://ermongroup.github.io/cs228-notes/representation/undirected/#conditional-random-fields).*\n",
    "\n",
    "##### Features\n",
    "\n",
    "- Assume the factors $\\phi_{c}(x_{c}, y_{c})$ are of the following form:\n",
    "$$\\phi_{c}(x_{c}, y_{c}) = \\exp(w_{c}^{T} f_{c}(x_{c}, y_{c}))$$\n",
    "    - Where $f_{c}(x_{c}, y_{c})$ can be an arbitrary set of features describing the compatibility between $x_{c}$ and $y_{c}$.\n",
    "    - Where $w_{c}^{T}$ is the transposed weight matrix.\n",
    "- Accordingly, $f_{c}(x_{c}, y_{c})$ allows arbitrarily complex features.\n",
    "    - e.g., $f(x, y_{i})$ are features that depend on the entirety of input samples $x$.\n",
    "    - e.g., $f(y_{i}, y_{i + 1})$ are features that depend on successive pairs of output labels $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditonal Random Fields vs. Markov Random Fields - Why is a conditional random field a special case of Markov random fields?\n",
    "\n",
    "- If we were to model $p(x, y)$ using a Markov random field, then we need to fit two probability distributions to the data: $p(y \\mid x)$ and $p(x)$.\n",
    "    - *Remember Baye's Rule*: $p(x, y) = p(y \\mid x) \\cdot p(x)$\n",
    "- However, if all we are interested in is predicting $y$ given $x$, then modeling $p(x)$ is expensive and unnecessary.\n",
    "$$\\text{Prediction} \\implies \\text{CRF} > \\text{MRF}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor Graphs - What is a factor graph? Why does a factor graph exist?\n",
    "\n",
    "![Factor Graph](images/MRF_3.png)\n",
    "\n",
    "- A **factor graph** is a bipartite graph where one group is the variables in the distribution being modeled, and the other group is the factors defined on these variables.\n",
    "    - *Edges Between Factors and Variables*\n",
    "- **Side Note**: A **bipartite graph** is a graph whose vertices are divided into two disjoint and independent sets.\n",
    "    - *Set 1: Variables*\n",
    "    - *Set 2: Factors*\n",
    "- **Important Note**: Use a factor graph to identify what variables a factor depends on when computing probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference <span name=\"S03\"></span>\n",
    "\n",
    "- **Problem**: *Given a probabilistic model, how do we obtain answers to relevant questions about the world?*\n",
    "    - **Marginal Inference**: *What is the probability of a given variable in our model after we sum everything else out?*\n",
    "$$p(y = 1) = \\sum_{x_{1}} \\sum_{x_{2}} \\cdots \\sum_{x_{n}} p(y = 1, x_{1}, x_{2}, ..., x_{n})$$\n",
    "        - e.g., What is the overall probability that an email is spam?\n",
    "        - *Perspective*: We desire to infer the general probability of some real-world phenomenon being observed.\n",
    "            - i.e., You care more about spam as a whole than specific instances of spam.\n",
    "    - **Maximum A Posteriori**: *What is the most likely assignment of variables?*\n",
    "$$\\max_{x_{1}, ..., x_{n}} p(y = 1, x_{1}, x_{2}, ..., x_{n})$$\n",
    "        - e.g., What is the set of words such that an email has the maximum probability of being spam?\n",
    "        - *Perspective*: We desire to infer the set of conditions that maximizes the probability of some real-world phenomenon being observed.\n",
    "            - i.e., You care more about identifying indicators of spam than detecting spam.\n",
    "    - *Naive Complexity*: NP-Hard **(DIFFICULT PROBLEM)**\n",
    "- **Solution**: *Exact Inference Algorithms & Approximate Inference Algorithms*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vairable Elimination (Exact Inference Algorithm) <span name=\"S03-1\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation - Why does the variable elimination algorithm exist?\n",
    "\n",
    "- Let $x_{i}$ be a discrete random variable that takes $k$ possible values.\n",
    "- **Problem**: *Marginal Inference*\n",
    "$$p(y = 1) = \\sum_{x_{1}} \\sum_{x_{2}} \\cdots \\sum_{x_{n}} p(y = 1, x_{1}, x_{2}, ..., x_{n})$$\n",
    "- **Naive Solution's Time Complexity** (*Exponential*): $O(k^{n})$\n",
    "    - *See [Rule of Product in Combinatorics](https://en.wikipedia.org/wiki/Rule_of_product)*\n",
    "- **Variable Elimination Solution's Time Complexity** (*Non-Exponential*): $O(n \\cdot k^{M + 1})$\n",
    "    - *See Below*.\n",
    "$$\\therefore \\text{Variable Elimination Solution} \\ll \\text{Naive Solution}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factors - How should a probabilistic graphical model express a probability distribution?\n",
    "\n",
    "- **Assumption**: *Probabilistic Graphical Models = Product of Factors*\n",
    "$$p(x_{1}, ..., x_{n}) = \\prod_{c \\in C} \\phi_{c}(x_{c})$$\n",
    "- **Representation**: A factor can be represented as a *multi-dimensional table* with a cell for each assignment of $x_{c}$.\n",
    "- *Bayesian Networks*: $\\phi$ is Conditional Probability Distribution\n",
    "- *Markov Random Fields*: $\\phi$ is Potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor Product - What is the product operation?\n",
    "\n",
    "![Example of Factor Product](images/VE_1.png)\n",
    "\n",
    "- Let $A$, $B$, and $C$ be three disjoint sets of variables.\n",
    "- Let $\\phi_{1}(A, B)$ and $\\phi_{2}(B, C)$ be two factors.\n",
    "- Let $\\phi_{3}(A, B, C)$ be the **factor product**.\n",
    "$$\\phi_{3}(A, B, C) = \\phi_{1}(A, B) \\cdot \\phi_{2}(B, C)$$\n",
    "    - Where the two factors are multiplied for common values of $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factor Marginalization - What is the marginalization operation?\n",
    "\n",
    "![Example of Factor Marginalization](images/VE_2.png)\n",
    "\n",
    "- Let $A$, and $B$ be two disjoint sets of variables.\n",
    "- Let $\\phi(A, B)$ be a factor.\n",
    "- Let $\\tau(A)$ be the **factor marginalization** of $B$ in $\\phi$.\n",
    "$$\\tau(A) = \\sum_{B} \\phi(A, B)$$\n",
    "- **Important Note**: $\\tau$ does not need necessarily correspond to a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering - What is an ordering?\n",
    "\n",
    "- An **ordering** $O$ is the sequence of variables by which they will be eliminated.\n",
    "- Although any ordering can be used, different orderings may dramatically alter the running time of the variable elimination algorithm.\n",
    "- **Important Note**: *Finding Best Ordering = NP-Hard*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm - How does the variable elimination algorithm work?\n",
    "\n",
    "- For each variable $X_{i}$ (ordered according to $O$),\n",
    "    1. Multiply all factors $\\Phi_{i}$ containing $X_{i}$.\n",
    "    2. Marginalize out $X_{i}$ to obtain a new factor $\\tau$.\n",
    "    3. Replace the factors $\\Phi_{i}$ with $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Complexity - What is the time complexity of variable elimination?\n",
    "\n",
    "- **Time Complexity**: $O(n \\cdot k^{M + 1})$\n",
    "    - Where $n$ is the number of variables.\n",
    "    - Where $M$ is the maximum number of dimensions of any factor $\\tau$ formed during the elimination process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering Heuristics - How should you choose an ordering for variable elimination?\n",
    "\n",
    "- **Minimum Neighbors**: Choose a variable with the fewest dependent variables.\n",
    "- **Minimum Weight**: Choose variables to minimize the product of the cardinalities of its dependent variables.\n",
    "- **Minimum Fill**: Choose vertices to minimize the size of the factor that will be added to the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence - How do you perform marginal inference given some evidence using variable elimination?\n",
    "\n",
    "- Given a probability distribution $P(X, Y, E)$ with unobserved variables $X$, query variables $Y$, and observed evidence variables $E$, $P(Y \\mid E = e)$ can be calculated using variable elimination.\n",
    "$$P(Y \\mid E = e) = \\frac{P(Y, E = e)}{P(E = e)}$$\n",
    "\n",
    "##### Variable Elimination with Evidence\n",
    "\n",
    "1. Set every factor $\\phi(X', Y', E')$ with values specified by $E = e$.\n",
    "2. Compute $P(Y, E = e)$ by performing variable elimination over $X$.\n",
    "3. Compute $P(E = e)$ by performing variable elimination over $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Problem 1 - Variable Elimination\n",
    "\n",
    "![Problem 1 - Variable Elimination](images/VE_P1.png)\n",
    "\n",
    "- A Bayesian network that models a student's grade on an exam:\n",
    "    - $g$ is a ternary variable of the student's grade.\n",
    "    - $d$ is a binary variable of the exam's difficulty.\n",
    "    - $i$ is a binary variable of the student's intelligence.\n",
    "    - $l$ is a binary variable of the quality of a reference letter from the professor who taught the course.\n",
    "    - $s$ is a binary variable of the student's SAT score.\n",
    "$$p(l, g, i, d, s) = p(l \\mid g) \\cdot p(s \\mid i) \\cdot p(i) \\cdot p(g \\mid i, d) \\cdot p(d)$$\n",
    "\n",
    "##### Question (Marginal Inference)\n",
    "\n",
    "- What is the probability distribution of the quality of a reference letter from the professor who taught the course?\n",
    "$$p(l) = \\sum_{g} \\sum_{i} \\sum_{d} \\sum_{s} p(l, g, i, d, s)$$\n",
    "\n",
    "##### Solution (Variable Elimination)\n",
    "\n",
    "1. Order the variables according to the topological sort of the Bayesian network.\n",
    "$$d, i, s, g$$\n",
    "2. Eliminate $d$ with a new factor $\\tau_{1}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tau_{1}(g, i) &= \\sum_{d} p(g \\mid i, d) \\cdot p(d) \\\\\n",
    "p(l, g, i, s) &= p(l \\mid g) \\cdot p(s \\mid i) \\cdot p(i) \\cdot \\tau_{1}(g, i)\n",
    "\\end{align}\n",
    "$$\n",
    "3. Eliminate $i$ with a new factor $\\tau_{2}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tau_{2}(g, s) &= \\sum_{i} p(s \\mid i) \\cdot p(i) \\cdot \\tau_{1}(g, i) \\\\\n",
    "p(l, g, s) &= p(l \\mid g) \\cdot \\tau_{2}(g, s)\n",
    "\\end{align}\n",
    "$$\n",
    "4. Eliminate $s$ with a new factor $\\tau_{3}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tau_{3}(g) &= \\sum_{s} \\tau_{2}(g, s) \\\\\n",
    "p(l, g) &= p(l \\mid g) \\cdot \\tau_{3}(g)\n",
    "\\end{align}\n",
    "$$\n",
    "5. Eliminate $g$ with a new factor $\\tau_{4}$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tau_{4}(l) &= \\sum_{g} p(l \\mid g) \\cdot \\tau_{3}(g) \\\\\n",
    "p(l) &= \\tau_{4}(l)\n",
    "\\end{align}\n",
    "$$\n",
    "6. Expanding $\\tau_{i}$:\n",
    "$$p(l) = \\sum_{g} p(l \\mid g) \\cdot \\sum_{s} \\sum_{i} p(s \\mid i) \\cdot p(i) \\cdot \\sum_{d} p(g \\mid i, d) \\cdot p(d)$$\n",
    "\n",
    "##### Time Complexity\n",
    "\n",
    "- **Naive Solution**: $O(k^{4})$\n",
    "- **Variable Elimination Solution**: $O(4 \\cdot k^{3})$\n",
    "    - Step 2. takes $O(k^{3})$ steps as the factor product $p(g \\mid i, d) \\cdot p(d)$ has a $3$-dimensional table representation, and the factor marginalization of $d$ can execute concurrently with the factor product.\n",
    "    - Step 3. takes $O(k^{3})$ steps as the factor product $p(s \\mid i) \\cdot p(i) \\cdot \\tau_{1}(g, i)$ has a $3$-dimensional table representation, and the factor marginalization of $i$ can execute concurrently with the factor product.\n",
    "    - Step 4. takes $O(k)$ steps for the factor marginalization of $s$.\n",
    "    - Step 5. takes $O(k^{2})$ steps as the factor product $p(l \\mid g) \\cdot \\tau_{3}(g)$ has a $2$-dimensional table representation, and the factor marginalization of $g$ can execute concurrently with the factor product.\n",
    "    - As $O(k^{3})$ is the largest step, with $4$ steps, the time complexity is at most $O(4 \\cdot k^{3})$.\n",
    "    - **Thus, with $n = 4$ and $M = 2$, the time complexity is at most $O(n \\cdot k^{M + 1}) = O(4 \\cdot k^{3})$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Inference <span name=\"S03-2\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview - What is MAP inference?\n",
    "\n",
    "- *See [Inference](#S03).*\n",
    "- Given a probabilistic graphical model $p(x_{1}, ..., x_{n}) = \\prod_{c \\in C} \\phi_{c}(x_{c})$, MAP inference corresponds to the following optimization problem:\n",
    "$$\\max_{x} \\log p(x) = \\max_{x} \\sum_{c \\in C} \\theta_{c}(x_{c}) - \\log Z$$\n",
    "    - Where $\\theta_{c}(x_{c}) = \\log \\phi_{c}(x_{c})$.\n",
    "    \n",
    "##### Derivation - Why is the MAP inference optmization problem expressed the way it is?\n",
    "\n",
    "\n",
    "1. All probabilistic graphical models (as BNs and CRFs are special cases of MRFs) have the following representation:\n",
    "$$p(x) = \\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}(x_{c})$$\n",
    "    - Where $Z$ is NP-Hard.\n",
    "2. MAP inference desires to infer the set of conditions that maximizes the probability of some real-world phenomenon being observed.\n",
    "$$\\max_{x} p(x) = \\max_{x} \\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}(x_{c})$$\n",
    "3. As $Z$ is expensive to calculate, maximize $\\log p(x)$ instead of $p(x)$.\n",
    "$$\\max_{x} \\log p(x) = \\max_{x} \\log \\left[ \\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}(x_{c}) \\right]$$\n",
    "4. Simplify using logarithmic identities.\n",
    "    - $\\log(x \\times y) = \\log(x) + \\log(y)$\n",
    "    - $\\log(x \\div y) = \\log(x) - \\log(y)$\n",
    "$$\\max_{x} \\log p(x) = \\max_{x} \\left[ \\sum_{c} \\log \\phi_{c}(x_{c}) - \\log Z \\right]$$\n",
    "5. Simplify using maximum identities.\n",
    "    - $\\max_{x}(x \\pm 1) = \\max_{x}(x) \\pm 1$\n",
    "$$\\max_{x} \\log p(x) = \\max_{x} \\sum_{c} \\log \\phi_{c}(x_{c}) - \\log Z$$\n",
    "6. Let $\\theta_{c}(x_{c}) = \\log \\phi_{c}(x_{c})$.\n",
    "$$\\max_{x} \\log p(x) = \\max_{x} \\sum_{c \\in C} \\theta_{c}(x_{c}) - \\log Z$$\n",
    "\n",
    "\n",
    "- As $\\log Z$ is outside the scope of the maximization, if you desire to infer the set of conditions that maximizes the probability of some real-world phenomenon being observed, then solve the following optimization problem:\n",
    "$$\\arg \\max_{x} \\log p(x) = \\arg \\max_{x} \\sum_{c \\in C} \\theta_{c}(x_{c})$$\n",
    "- **Important Note 1**: Without $Z$, this optimization problem suggests that MAP inference is computationally cheaper than marginal inference questions.\n",
    "- **Important Note 2**: As maximization and summation both distribute over products, techniques used to solve marginal inference problems can be used to solve MAP inference problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Cuts - How can MAP inference problems be solved using graph cuts?\n",
    "\n",
    "- A **graph cut** of an undirected graph $G = (V, E)$ is a partition of $V$ into two disjoint sets $V_{s}$ and $V_{t}$.\n",
    "- The **min-cut** problem is to find the partition $V_{s}, V_{t}$ that minimize the cost of the graph cut.\n",
    "    - The cost of a graph cut is the sum of the nonnegative costs of the edges that cross between the two partitions:\n",
    "$$\\text{cost}(V_{s}, V_{t}) = \\sum_{v_{1} \\in V_{s}, v_{2} \\in V_{t}} \\text{cost}(v_{1}, v_{2})$$\n",
    "    - **Time Complexity 1**: $O(\\lvert E \\rvert \\lvert V \\rvert \\log \\lvert V \\rvert)$\n",
    "    - **Time Complexity 2**: $O({\\lvert V \\rvert}^{3})$\n",
    "- A MAP inference problem can be reduced into the min-cut problem in certain restricted cases of MRFs with binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Programming - How can MAP inference problems be solved using linear programming?\n",
    "\n",
    "- An approximate approach to computing the MAP values is to use Integer Linear Programming by introducing:\n",
    "    - An indicator variable per variable in the PGM.\n",
    "    - An indicator variable per edge/clique in the PGM.\n",
    "    - Constraints on consistent values in cliques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Search - How can MAP inference problems be solved using local search?\n",
    "\n",
    "- A heuristic solution that starts with an arbitrary assignment and performs modifications on the joint assignment that locally increase the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branch and Bound - How can MAP inference problems be solved using branch and bound?\n",
    "\n",
    "- An exhaustive solution that searches over the space of assignments while pruning branches that can be provably shown not to contain a MAP assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated Annealing - How can MAP inference problems be solved using simulated annealing?\n",
    "\n",
    "- A sampling solution that expresses a probability distribution with the following:\n",
    "$$p_{t}(x) \\propto \\exp\\left( \\frac{1}{t} \\sum_{c \\in C} \\theta_{c}(x_{c}) \\right)$$\n",
    "    - Where $t$ is temperature.\n",
    "        - $t \\to \\infty$ $\\implies$ $p_{t}$ approaches a continuous uniform distribution.\n",
    "        - $t \\to 0$ $\\implies$ $p_{t}$ approaches a continuous exponential distribution with a significant peak of $\\arg \\max_{x} \\sum_{c \\in C} \\theta_{c}(x_{c})$.\n",
    "- As the peak is a MAP assignment, a sampling algorithm starting with a high temperature which gradually decreases can eventually find the peak, given a sufficiently slow cooling rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling-Based Inference <span name=\"S03-3\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation - Why does sampling-based inference algorithms exist?\n",
    "\n",
    "- **Exact Inference Algorithms**: Slow/NP-Hard\n",
    "- **Approximate Inference Algorithms**: Marginal Inference, MAP Inference, Expectations\n",
    "\n",
    "##### Expectations $\\mathbb{E}[f(X)]$ - Why do we want to estimate expectations of random variables?\n",
    "\n",
    "- Abstractly, approximate inference algorithms want to estimate the probability of some real-world phenomenon.\n",
    "- Mathematically, estimating a probability $p(x)$ is a **SPECIALIZATION** of estimating an expectation $\\mathbb{E}_{x \\sim p}[f(x)] = \\sum_{x} f(x)p(x)$\n",
    "- If $f(x) = \\mathbb{I}_{\\lvert x \\rvert}$, where $\\mathbb{I}_{\\lvert x \\rvert}$ is an indicator function for event $x$,\n",
    "$$\\mathbb{E}_{x \\sim p}[\\mathbb{I}_{\\lvert x \\rvert}] = p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Sampling - How do you sample a discrete CPD?\n",
    "\n",
    "\n",
    "1. Let $p$ be a multinomial probability distribution with event values $\\{x^{1}, ..., x^{k}\\}$ and event probabilities $\\{\\theta_{1}, ..., \\theta_{k}\\}$.\n",
    "2. Generate a sample $s$ uniformly from the interval $[0, 1]$.\n",
    "3. Partition the interval into $k$ subintervals:\n",
    "$$[0, \\theta_{1}), [\\theta_{1}, \\theta_{1} + \\theta_{2}), ..., \\left[ \\sum_{j = 1}^{i - 1} \\theta_{j}, \\sum_{j = 1}^{i} \\theta_{j} \\right)$$\n",
    "4. If $s$ is in the $i$th interval, then the sampled value is $x^{i}$.\n",
    "\n",
    "\n",
    "- **Time Complexity**: $O(\\log k)$ - *Using Binary Search*\n",
    "- *Remember Baye's Rule*: $p(y \\mid x) = \\frac{p(x, y)}{p(x)}$\n",
    "    - $p(x, y)$ is a multinomial probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Sampling - How do you sample a discrete Bayesian network?\n",
    "\n",
    "\n",
    "1. Let $G$ be a Bayesian network representing a probability distribution $p(x_{1}, ..., x_{n})$.\n",
    "2. Sample the variables in a topological order.\n",
    "3. Sample the successor variables by conditioning these node's CPDs to the values sampled by their ancestors.\n",
    "4. Repeat until all $n$ variables have been sampled.\n",
    "\n",
    "\n",
    "- **Time Complexity**: $O(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Integration/Estimation - How do you take a large number of samples to estimate expectations?\n",
    "\n",
    "- *Monte Carlo $\\approx$ Large Number of Samples*\n",
    "$$\\mathbb{E}_{x \\sim p}[f(x)] \\approx I_{T} = \\frac{1}{T} \\sum_{t = 1}^{T} f(x^{t})$$\n",
    "    - Where $x^{1}, ..., x^{T}$ are [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) samples drawn according to $p$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{x^{1}, ..., x^{T} \\sim^{\\text{i.i.d.}} p}[I_{T}] &=  \\mathbb{E}_{x \\sim p}[f(x)] \\\\\n",
    "\\text{Var}_{x^{1}, ..., x^{T} \\sim^{\\text{i.i.d.}} p}[I_{T}] &= \\frac{1}{T} \\text{Var}_{x \\sim p}[f(x)]\n",
    "\\end{align}\n",
    "$$\n",
    "    - Where the Monte Carlo estimate $I_T$\n",
    "    \n",
    "##### Implications - What is important about Monte Carlo estimations?\n",
    "\n",
    "1. $I_{T}$ is an unbiased estimator for $\\mathbb{E}_{x \\sim p}[f(x)]$.\n",
    "2. Referencing the [Weak Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), if $T \\to \\infty$, then $I_{T} \\to \\mathbb{E}_{x \\sim p}[f(x)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rejection Sampling - How does rejection sampling work?\n",
    "\n",
    "- Compute a target probability distribution $p(x)$ by sampling a proposal probability distribution $q(x)$, rejecting samples inconsistent with $p(x)$, and applying the Monte Carlo estimation.\n",
    "    - **Examples**: *See [Rejection Sampling](https://ermongroup.github.io/cs228-notes/inference/sampling/)*\n",
    "    - **Disadvantage**: *Ignores Many Samples*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance Sampling - How does importance sampling work?\n",
    "\n",
    "- Compute a target probability distribution $p(x)$ by sampling a proposal probability distribution $q(x)$, reweighing samples with $w(x) = \\frac{p(x)}{q(x)}$, and applying the Monte Carlo estimation.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{x \\sim p}[f(x)] &= \\sum_{x} f(x)p(x) \\\\\n",
    "&= \\sum_{x} f(x)\\frac{p(x)}{q(x)}q(x) \\\\\n",
    "&= \\mathbb{E}_{x \\sim q}[f(x)w(x)] \\\\\n",
    "&\\approx \\frac{1}{T} \\sum_{t = 1}^{T} f(x^{t})w(x^{t})\n",
    "\\end{align}\n",
    "$$\n",
    "    - **Examples**: *See [Importance Sampling](https://ermongroup.github.io/cs228-notes/inference/sampling/)*\n",
    "    - **Advantage**: *Uses All Samples*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Importance Sampling - How does normalized importance sampling work?\n",
    "\n",
    "1. Let $p(x)$ be unknown.\n",
    "2. Let $\\tilde{p}(x) = Z \\cdot p(x)$ be known.\n",
    "3. The weight $w(x) = \\frac{\\tilde{p}(x)}{q(x)}$ is invalid for unnormalized importance sampling.\n",
    "4. The **normalizing constant** of the distribution $\\tilde{p}(x)$ is the following:\n",
    "$$\\mathbb{E}_{x \\sim q}[w(x)] = \\sum_{x} q(x)\\frac{\\tilde{p}(x)}{q(x)} = \\sum_{x} \\tilde{p}(x) = Z$$\n",
    "5. The **normalized importance sampling estimator** is the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{x \\sim p}[f(x)] &= \\sum_{x} f(x)p(x) \\\\\n",
    "&= \\sum_{x} f(x)\\frac{p(x)}{q(x)}q(x) \\\\\n",
    "&= \\frac{1}{Z} \\sum_{x} f(x)\\frac{\\tilde{p}(x)}{q(x)}q(x) \\\\\n",
    "&= \\frac{1}{Z} \\mathbb{E}_{x \\sim q}[f(x)w(x)] \\\\\n",
    "&= \\frac{\\mathbb{E}_{x \\sim q}[f(x)w(x)]}{\\mathbb{E}_{x \\sim q}[w(x)]}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Chain - What is a Markov chain?\n",
    "\n",
    "- **Markov Chain**: A sequence of random variables $S_{0}, S_{1}, S_{2}, ...$ with each random variable $S_{i} \\in \\{1, 2, ..., d\\}$ taking one of $d$ possible values.\n",
    "    - *Initial State*: $P(S_{0})$\n",
    "    - *Subsequent States*: $P(S_{i} \\mid S_{i - 1})$\n",
    "- **Markov Assumption**: $S_{i}$ cannot depend directly on $S_{j}$ where $j < i - 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationary Distribution - Why is it important for a stationary distribution to exist?\n",
    "\n",
    "- Let $T_{ij} = P(S_{\\text{new}} = i \\mid S_{\\text{prev}} = j)$ be a $d \\times d$ transition probability matrix.\n",
    "- If the initial state $S_{0}$ is drawn from a vector probabilities $p_{0}$, the probability $p_{t}$ of ending in **EACH STATE** after $t$ steps is the following:\n",
    "$$p_{t} = T^{t} p_{0}$$\n",
    "    - Where $T^{t}$ is matrix exponentiation.\n",
    "- **Stationary Distribution**: If it exists, the limit $\\pi = \\lim_{t \\to \\infty} p_{t}$.\n",
    "- **Important Note 1**: A Markov chain whose states are joint assignments to the variables in a probabilistic graphical model $p$ has a stationary distribution equal to $p$.\n",
    "\n",
    "##### Existence of Stationary Distribution\n",
    "\n",
    "- **Irreducibility**: It is possible to get from any state $x$ to any other state $x'$ with probability $>0$ in a finite number of steps.\n",
    "- **Aperiodicity**: It is possible to return to any state at any time, i.e. there exists an $n$ such that for all $i$ and all $n' \\ge n$, $P(s_{n'} = i \\mid s_{0} = i) > 0$.\n",
    "- **Important Note 2**: An irreducible and aperiodic finite-state Markov chain has a stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Chain Monte Carlo - How do you sample from a MCMC?\n",
    "\n",
    "1. Let $T$ be a **transition operator** specifying a Markov chain whose stationary distribution is $p$.\n",
    "2. Le $x_{0}$ be an initial assignment to the variables of $p$.\n",
    "3. Run the Markov chain from $x_{0}$ for $B$ *burn-in* steps.\n",
    "    - If $B$ is sufficiently large, $\\pi \\to p$.\n",
    "4. Run the Markov chain for $N$ *sampling* steps and collect all the states that it visits.\n",
    "    - The collection of states form samples from $p$.\n",
    "\n",
    "##### Applications of Markov Chain Monte Carlo\n",
    "\n",
    "1. Use samples for Monte Carlo integration to estimate expectations.\n",
    "2. Use samples to perform marginal inference.\n",
    "3. Use the sample with the highest probability to perform MAP inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gibbs Sampling - How do you construct a MCMC?\n",
    "\n",
    "\n",
    "1. Let $x_{1}, ..., x_{n}$ be an ordered set of variables.\n",
    "2. Let $x^{0} = (x_{1}^{0}, ..., x_{n}^{0})$ be a starting configuration.\n",
    "3. Repeat until convergence for $t = 1, 2, ...$,\n",
    "    1. Set $x \\gets x^{t - 1}$.\n",
    "    2. For each variable $x_{i}$,\n",
    "        1. Sample $x_{i}' \\sim p(x_{i} \\mid x_{-i})$.\n",
    "            - Where $x_{-i}$ is all variables in $x$ except $x_{i}$\n",
    "        2. Update $x \\gets (x_{1}, ..., x_{i}', ..., x_{n})$.\n",
    "    3. Set $x^{t} \\gets x$\n",
    "    \n",
    "\n",
    "- **Important Note 1**: When $x_{i}$ is updated, its new value is immediately used for sampling other variables $x_{j}$.\n",
    "- **Important Note 2**: Every iteration of $x^{t}$ is a new sample from $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning <span name=\"S04\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem**: *Given a dataset $D$ of $m$ i.i.d. samples from some underlying distribution $p^{\\ast}$, how do you fit the best model, given a family of models $M$, to make useful predictions?*\n",
    "    - **Parameter Learning**: *Where the graph structure is known, and we want to estimate the factors.*\n",
    "    - **Structure Learning**: *Where we want to estimate the graph, i,e. determine from data how the variables depend on each other.*\n",
    "- **Solution**: *Best Approximation of $p^{\\ast}$*\n",
    "    - **Density Estimation**: *We are interested in the full distribution.*\n",
    "    - **Specific Prediction Tasks**: *We are using the distribution to make a prediction.*\n",
    "        - e.g. Is this email spam or not?\n",
    "    - **Structure or Knowledge Discovery**: *We are interested in the model itself.*\n",
    "        - e.g. How do some genes interact with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation <span name=\"S04-01\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation - Why does maximum likelihood estimation exist?\n",
    "\n",
    "- **Goal**: How do we approximate $p$ as close as possible to $p^{\\ast}$?\n",
    "- **Approach**: When the KL divergence between $p$ and $p^{\\ast}$ is minimal, $p$ is as close as possible to $p^{\\ast}$.\n",
    "\n",
    "\n",
    "##### KL Divergence - What is KL divergence?\n",
    "\n",
    "- **KL Divergence**: How different is one probability distribution from another probability distribution?\n",
    "$$KL(p^{\\ast} \\parallel p) = \\sum_{x} p^{\\ast}(x) \\log \\frac{p^{\\ast}(x)}{p(x)} = -H(p^{\\ast}) - \\mathbb{E}_{x \\sim p^{\\ast}}[\\log p(x)]$$\n",
    "\n",
    "##### Minimal KL Divergence - When is KL divergence minimal?\n",
    "\n",
    "- **General Idea**: *Minimizing KL Divergence* $\\Longleftrightarrow$ *Maximizing Likelihood*\n",
    "$$\\min KL(p^{\\ast} \\parallel p) \\Longleftrightarrow \\max \\mathbb{E}_{x \\sim p^{\\ast}}[\\log p(x)]$$\n",
    "- Because $p^{\\ast}$ is unknown, approximate the log-likelihood with the emperical log-likelihood using a Monte-Carlo estimate.\n",
    "$$\\mathbb{E}_{x \\sim p^{\\ast}}[\\log p(x)] \\approx \\frac{1}{\\lvert D \\rvert} \\sum_{x \\in D} \\log p(x)$$\n",
    "\n",
    "##### Maximum Likelihood Learning - How do you fit the best model using maximum likelihood learning?\n",
    "\n",
    "- Given a family of models $M$, to fit the best model $p$, compute the following.\n",
    "$$\\max_{p \\in M} \\mathbb{E}_{x \\sim p^{\\ast}}[\\log p(x)] \\approx \\max_{p \\in M} \\frac{1}{\\lvert D \\rvert} \\sum_{x \\in D} \\log p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition - What is maximum likelihood estimation?\n",
    "\n",
    "- **Maximum Likelihood Estimation**: Given a data set $D$, choose parameters $\\hat{\\theta}$ that satisfy the following.\n",
    "$$\\max_{\\theta in \\Theta} L(\\theta, D)$$\n",
    "    - i.e., Maximize the parameters $\\theta$ to best fit the data set $D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function - What is a loss function?\n",
    "\n",
    "- **Loss Function** ($L(x, p)$): A measure of the loss that a model distribution $p$ makes on a particular instance $x$.\n",
    "    - e.g., *MLE Loss Function*: $L(x, p) = -\\log p(x)$\n",
    "- **Important Note**: Assuming instances are sampled from some distribution $p^{\\ast}$, to fit the best model, **MINIMIZE** the expected loss.\n",
    "$$\\mathbb{E}_{x \\sim p^{\\ast}}[L(x, p)] \\approx \\frac{1}{\\lvert D \\rvert} \\sum_{x \\in D} L(x, p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Function - What is a likelihood function?\n",
    "\n",
    "- **Likelihood Function** ($L(\\theta, D)$): The probability of observing the i.i.d. samples $D$ for all permissible values of the parameters $\\theta$.\n",
    "\n",
    "##### Example - Likelihood Function\n",
    "\n",
    "1. Let $p(x)$ be a probability distribution where $x \\in \\{h, t\\}$ such that $p(x = h) = \\theta$ and $p(x = t) = 1 - \\theta$.\n",
    "2. Let $D = \\{h, h, t, h, t\\}$ be observed i.i.d. samples.\n",
    "4. Accordingly, $p(x)$ models the outcome of a biased coin where parameter $\\theta$ represents the probability of flipping heads and $1 - \\theta$ represents the probability of flipping tails.\n",
    "3. Express the likelihood function as the following.\n",
    "$$L(\\theta, D) = \\theta \\cdot \\theta \\cdot (1 - \\theta) \\cdot \\theta \\cdot (1 - \\theta) = \\theta^{3} \\cdot (1 - \\theta)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Learning - How does maximum likelihood learning estimate the CPDs in Bayesian networks?\n",
    "\n",
    "\n",
    "1. Let $p(x) = \\prod_{i = 1}^{n} \\theta_{x_{i} \\mid x_{pa(i)}}$ be a Bayesian network.\n",
    "    - Where $\\theta_{x_{i} \\mid x_{pa(i)}}$ are parameters (CPDs) with **UNKNOWN VALUES**.\n",
    "2. Let $D = \\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$ be i.i.d. samples.\n",
    "3. Let $L(\\theta, D) = \\prod_{i = 1}^{n} \\prod_{j = 1}^{m} \\theta_{x_{i}^{j} \\mid x_{pa(i)}^{j}}$ be the likelihood function.\n",
    "4. Log and collect like terms of the likelihood function.\n",
    "$$\\log L(\\theta, D) = \\sum_{i = 1}^{n} \\sum_{x_{pa(i)}} \\sum_{x_{i}} \\#(x_{i}, x_{pa(i)}) \\cdot \\log \\theta_{x_{i} \\mid x_{pa(i)}}$$\n",
    "5. Maximize the (log) likelihood function by decomposing it into separate maximizations for the local conditional distributions.\n",
    "\n",
    "\n",
    "- **Important Note**: The maximum-likelihood estimates of the parameters (CPDs) have closed-form solutions.\n",
    "$$\\theta_{x_{i} \\mid x_{pa(i)}}^{\\ast} = \\frac{\\#(x_{i}, x_{pa(i)})}{\\#(x_{pa(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Learning <span name=\"S04-02\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation - What are some problems with maximum likelihood estimation?\n",
    "\n",
    "- A maximum likelihood estimate does not change as more data is observed because it assumes that the only source of uncertainty is explained by the parameters that are being fitted.\n",
    "- **Problem 1**: *Cannot Improve Confidence*\n",
    "- **Problem 2**: *Cannot Incorporate Prior Knowledge*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions - What are a *prior* and a *posterior*?\n",
    "\n",
    "- **Bayesian Learning**: Explicitly model uncertainty over both variables $X$ and parameters $\\theta$ by letting parameters be random variables.\n",
    "- A **prior** is the earlier probability distribution of parameter $\\theta$ **BEFORE** observing data $D$.\n",
    "- A **posterior** is the later probability distribution of parameter $\\theta$ **AFTER** observing data $D$.\n",
    "$$p(\\theta \\mid D) = \\frac{p(D \\mid \\theta) p(\\theta)}{p(D)} \n",
    "\\propto p(D \\mid \\theta) p(\\theta)$$\n",
    "$$posterior \\propto likelihood \\times prior$$\n",
    "- **Important Note 1**: Bayes'rule allows prior knowledge to be incorporated into a model's parameters.\n",
    "- **Important Note 2**: Using Bayes' rule, the numerator is easy to calculate, but the denominator is difficult to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate Priors - What is a *conjugate prior*?\n",
    "\n",
    "- A parametric family $\\phi$ is **conjugate** for the likelihood $P(D \\mid \\theta)$ if:\n",
    "$$P(\\theta) \\in \\phi \\implies P(\\theta \\mid D) \\in \\phi$$\n",
    "- **Important Note**: If the normalizing constant of $\\phi$ is known, then the denominator in Bayes' rule is easy to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta Distribution - What is the Beta distribution?\n",
    "\n",
    "![Examples of Beta Distribution](images/BL_1.png)\n",
    "\n",
    "- A **Beta distribution** is parameterized by two hyperparameters $\\alpha \\in \\mathbb{R}$, and $\\beta \\in \\mathbb{R}$ with the following continuous probability distribution.\n",
    "$$\\theta \\sim \\text{Beta}(\\alpha, \\beta) \\implies p(\\theta) = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "    - Where the constant $\\alpha$ intuitively corresponds to the number of **SUCCESSES** before observing new data.\n",
    "    - Where the constant $\\beta$ intuitively corresponds to the number of **FAILURES** before observing new data.\n",
    "    - Where the constant $B(\\alpha, \\beta)$ is a normalizing constant defined by the following.\n",
    "$$B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$$\n",
    "    - Where the Gamma function $\\Gamma(x)$ is the continuous generalization of the factorial function defined by the following.\n",
    "$$\\Gamma(x) = \\int_{0}^{\\infty} t^{x - 1} e^{-t} dt$$\n",
    "- **Mean**: $$\\text{mean}[X] = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
    "- **Variance**: $$\\text{var}[X] = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^{2} (\\alpha + \\beta + 1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate Priors and Beta Distribution - How do you calculate a posterior with data observed from a binary process?\n",
    "\n",
    "- The beta distribution is the conjugate prior for the following probability distributions:\n",
    "    - **Bernoulli**: A discrete Bernoulli random variable, $X$, is the outcome from a single experiment from which this outcome is classified as either a success, $X = 1$ with probability $p$, or a failure, $X = 0$ with probability $1 - p$.\n",
    "    - **Binomial**: A discrete binomial random variable, $X$, is the number of successful outcomes from a sequence of $n$ independent experiments in which each experiment has an outcome classified as either a success with probability $p$ or a failure with probability $1 - p$\n",
    "    - **Geometric**: A discrete geometric random variable, $X$, is the number of Bernoulli trials with probability $p$ needed to get one success.\n",
    "    - **Negative Binomial**: A discrete negative binomial random variable, $X$, is the number of successes in a sequence of independent and identically distributed Bernoulli trials before a specified (non-random) number of failures.\n",
    "- **Important Note**: To best fit a binary model with a probability distribution $p$, the beta distribution can be used by the following.\n",
    "    1. Assign $\\text{Beta}(\\alpha, \\beta)$ as a prior to $p$.\n",
    "    2. Observe data generated by a binary process.\n",
    "        - If $X \\sim \\text{Bernoulli}(\\theta)$, then the posterior is $\\text{Beta}(\\alpha + 1, \\beta)$ or $\\text{Beta}(\\alpha, \\beta + 1)$.\n",
    "        - If $X \\sim \\text{Binomial}(N, \\theta)$, then the posterior is $\\text{Beta}(\\alpha + X, \\beta + N - X)$.\n",
    "        - If $X \\sim \\text{Geometric}(N, \\theta)$, then the posterior is $\\text{Beta}(\\alpha + X, \\beta + 1)$.\n",
    "        - If $X \\sim \\text{Negative-Binomial}(R, \\theta)$, then the posterior is $\\text{Beta}(\\alpha + X, \\beta + R)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
